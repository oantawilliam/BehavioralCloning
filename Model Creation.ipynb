{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import csv\n",
    "import glob \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Default Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process default data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_csv = './default_data/driving_log.csv'\n",
    "path = './default_data/'\n",
    "\n",
    "car_images = []\n",
    "steering_angles = []\n",
    "\n",
    "# Get first track csv data\n",
    "with open(default_csv) as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    \n",
    "    for row in reader:\n",
    "            \n",
    "        steering_center = float(row[3])\n",
    "\n",
    "        # create adjusted steering measurements for the side camera images\n",
    "        correction = 0.2 # this is a parameter to tune\n",
    "        steering_left = steering_center + correction\n",
    "        steering_right = steering_center - correction\n",
    "\n",
    "        # read in images from center, left and right cameras\n",
    "        img_center = cv2.imread(path + row[0].strip())\n",
    "        img_left = cv2.imread(path + row[1].strip())\n",
    "        img_right = cv2.imread(path + row[2].strip())\n",
    "        \n",
    "        img_center = cv2.cvtColor(img_center, cv2.COLOR_BGR2RGB)\n",
    "        img_left = cv2.cvtColor(img_left, cv2.COLOR_BGR2RGB)\n",
    "        img_right = cv2.cvtColor(img_right, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # add images and angles to data set\n",
    "        car_images.extend([img_center, img_left, img_right])\n",
    "        steering_angles.extend([steering_center, steering_left, steering_right])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-759ff00639b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build training data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcar_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteering_angles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build training data set\n",
    "X_train_d = np.array(car_images)\n",
    "y_train_d = np.array(steering_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f87b5ddb470>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB1CAYAAABeSBpCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMqGBLiKSiTe6I9BzhtjQhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f87b5e18a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(car_images[0][60:-21,:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e32ff90add01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdefault_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"features\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mX_train_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"labels\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_train_d\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"default_train.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "default_train_data = {\"features\" : X_train_d,\"labels\" : y_train_d}\n",
    "pickle.dump(default_train_data, open( \"default_train.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Recorded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process CSV measurements file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21895\n"
     ]
    }
   ],
   "source": [
    "# Tracks directories\n",
    "first_track = './data/first_track'\n",
    "second_track = './data/second_track'\n",
    "\n",
    "log_file = 'driving_log.csv'\n",
    "imgs_folder = 'IMG'\n",
    "log_csv = []\n",
    "\n",
    "# List used to skip some data\n",
    "valid_folders = ['center_lane_1', \n",
    "                 'curves_smoothly_1_f_1', \n",
    "                 'curves_smoothly_1_f_2', \n",
    "                 'recovery_1',\n",
    "                 'center_lane_2', \n",
    "                 'curves_smoothly_2_f', \n",
    "                 'recovery_2']\n",
    "\n",
    "# Get first track csv data\n",
    "for file in os.listdir(first_track):\n",
    "#     if file in valid_folders:\n",
    "    # Build CSV path\n",
    "    csv_path = \"{}/{}/{}\".format(first_track, file, log_file)\n",
    "\n",
    "    # Load CSV data\n",
    "    with open(csv_path) as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        for line in reader:\n",
    "            log_csv.append(line)\n",
    "\n",
    "# Get second track csv data\n",
    "for file in os.listdir(second_track):\n",
    "#     if file in valid_folders:\n",
    "        \n",
    "    # Build CSV path\n",
    "    csv_path = \"{}/{}/{}\".format(second_track, file, log_file)\n",
    "\n",
    "    # Load CSV data\n",
    "    with open(csv_path) as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        for line in reader:\n",
    "            log_csv.append(line)\n",
    "\n",
    "print(len(log_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5315\n",
      "5315\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "measurements = []\n",
    "\n",
    "for line in log_csv:\n",
    "    # Get image path\n",
    "    source_path = line[0]\n",
    "    # Get image name\n",
    "    file_name = source_path.split('/')[-1]\n",
    "    # Change image path \n",
    "    current_path = './data/IMG/' + file_name\n",
    "    # Store Image\n",
    "    image = cv2.imread(current_path)\n",
    "    images.append(image)\n",
    "    \n",
    "    # Store Steering Angle\n",
    "    measurement = float(line[3])\n",
    "    measurements.append(measurement)\n",
    "\n",
    "# Build training data set\n",
    "X_train = np.array(images)\n",
    "y_train = np.array(measurements)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = {\"features\" : X_train,\"labels\" : y_train}\n",
    "pickle.dump(train_data, open( \"train.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train.p\", \"rb\") as train_file:\n",
    "    train_data = pickle.load(train_file)\n",
    "    \n",
    "X_train, y_train = train_data[\"features\"], train_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest Model Possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest Model Try Out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17516 samples, validate on 4379 samples\n",
      "Epoch 1/10\n",
      "17516/17516 [==============================] - 20s - loss: 1633333.1567 - val_loss: 1195.7598\n",
      "Epoch 2/10\n",
      "17516/17516 [==============================] - 20s - loss: 2928.4181 - val_loss: 8850.5901\n",
      "Epoch 3/10\n",
      "17516/17516 [==============================] - 20s - loss: 154086.9365 - val_loss: 19758618.6120\n",
      "Epoch 4/10\n",
      "17516/17516 [==============================] - 20s - loss: 559733.1857 - val_loss: 4464.4862\n",
      "Epoch 5/10\n",
      "17516/17516 [==============================] - 20s - loss: 374218.4078 - val_loss: 25923.1692\n",
      "Epoch 6/10\n",
      "17516/17516 [==============================] - 20s - loss: 292226.3216 - val_loss: 54597.8005\n",
      "Epoch 7/10\n",
      "17516/17516 [==============================] - 20s - loss: 203483.8062 - val_loss: 45812.6437\n",
      "Epoch 8/10\n",
      "17516/17516 [==============================] - 20s - loss: 414011.9327 - val_loss: 5974.0142\n",
      "Epoch 9/10\n",
      "17516/17516 [==============================] - 20s - loss: 355963.7128 - val_loss: 73384.6647\n",
      "Epoch 10/10\n",
      "17516/17516 [==============================] - 20s - loss: 259130.7647 - val_loss: 392413.5940\n"
     ]
    }
   ],
   "source": [
    "m1 = Sequential()\n",
    "m1.add(Flatten(input_shape=(160, 320, 3)))\n",
    "m1.add(Dense(1))\n",
    "\n",
    "m1.compile(loss='mse', optimizer='adam')\n",
    "m1.fit(X_train, y_train, validation_split=0.2, shuffle=True)\n",
    "\n",
    "m1.save('m1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Images in Range: 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17516 samples, validate on 4379 samples\n",
      "Epoch 1/10\n",
      "17516/17516 [==============================] - 20s - loss: 25.9219 - val_loss: 0.1310\n",
      "Epoch 2/10\n",
      "17516/17516 [==============================] - 20s - loss: 0.1302 - val_loss: 0.1292\n",
      "Epoch 3/10\n",
      "17516/17516 [==============================] - 21s - loss: 3.3529 - val_loss: 0.4656\n",
      "Epoch 4/10\n",
      "17516/17516 [==============================] - 20s - loss: 4.8874 - val_loss: 0.1863\n",
      "Epoch 5/10\n",
      "17516/17516 [==============================] - 21s - loss: 7.0445 - val_loss: 0.2356\n",
      "Epoch 6/10\n",
      "17516/17516 [==============================] - 21s - loss: 3.0626 - val_loss: 2.8542\n",
      "Epoch 7/10\n",
      "17516/17516 [==============================] - 20s - loss: 5.9324 - val_loss: 7.9464\n",
      "Epoch 8/10\n",
      "17516/17516 [==============================] - 20s - loss: 5.2262 - val_loss: 0.7999\n",
      "Epoch 9/10\n",
      "17516/17516 [==============================] - 20s - loss: 5.8150 - val_loss: 0.2200\n",
      "Epoch 10/10\n",
      "17516/17516 [==============================] - 20s - loss: 3.7678 - val_loss: 4.7489\n"
     ]
    }
   ],
   "source": [
    "m2 = Sequential()\n",
    "m2.add(Lambda(lambda x: x / 255.0, input_shape=(160, 320, 3)))\n",
    "m2.add(Flatten())\n",
    "m2.add(Dense(1))\n",
    "\n",
    "m2.compile(loss='mse', optimizer='adam')\n",
    "m2.fit(X_train, y_train, validation_split=0.2, shuffle=True)\n",
    "\n",
    "m2.save('m2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Center Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17516 samples, validate on 4379 samples\n",
      "Epoch 1/10\n",
      "17516/17516 [==============================] - 21s - loss: 4.4784 - val_loss: 16.1374\n",
      "Epoch 2/10\n",
      "17516/17516 [==============================] - 21s - loss: 1.9974 - val_loss: 8.7859\n",
      "Epoch 3/10\n",
      "17516/17516 [==============================] - 21s - loss: 3.7621 - val_loss: 1.8375\n",
      "Epoch 4/10\n",
      "17516/17516 [==============================] - 21s - loss: 4.2360 - val_loss: 0.4352\n",
      "Epoch 5/10\n",
      "17516/17516 [==============================] - 21s - loss: 2.3698 - val_loss: 5.8420\n",
      "Epoch 6/10\n",
      "17516/17516 [==============================] - 21s - loss: 5.0843 - val_loss: 1.3734\n",
      "Epoch 7/10\n",
      "17516/17516 [==============================] - 21s - loss: 1.8498 - val_loss: 7.1161\n",
      "Epoch 8/10\n",
      "17516/17516 [==============================] - 21s - loss: 3.4522 - val_loss: 4.0788s: 3.454\n",
      "Epoch 9/10\n",
      "17516/17516 [==============================] - 21s - loss: 3.1270 - val_loss: 2.9286\n",
      "Epoch 10/10\n",
      "17516/17516 [==============================] - 21s - loss: 1.5077 - val_loss: 0.7649\n"
     ]
    }
   ],
   "source": [
    "m3 = Sequential()\n",
    "m3.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160, 320, 3)))\n",
    "m3.add(Flatten())\n",
    "m3.add(Dense(1))\n",
    "\n",
    "m3.compile(loss='mse', optimizer='adam')\n",
    "m3.fit(X_train, y_train, validation_split=0.2, shuffle=True)\n",
    "\n",
    "m3.save('m3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/lenet-5.png?x64257)\n",
    "![](http://gpucomputing.shef.ac.uk/static/img/intro_dl_sharc_dgx1/mnist_lenet.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19286 samples, validate on 4822 samples\n",
      "Epoch 1/10\n",
      "19286/19286 [==============================] - 71s - loss: 0.3134 - val_loss: 0.0153\n",
      "Epoch 2/10\n",
      "19286/19286 [==============================] - 70s - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 3/10\n",
      "19286/19286 [==============================] - 69s - loss: 0.0082 - val_loss: 0.0120\n",
      "Epoch 4/10\n",
      "19286/19286 [==============================] - 70s - loss: 0.0071 - val_loss: 0.0119\n",
      "Epoch 5/10\n",
      "19286/19286 [==============================] - 70s - loss: 0.0064 - val_loss: 0.0127\n",
      "Epoch 6/10\n",
      "19286/19286 [==============================] - 70s - loss: 0.0058 - val_loss: 0.0134\n",
      "Epoch 7/10\n",
      "19286/19286 [==============================] - 70s - loss: 0.0055 - val_loss: 0.0136\n",
      "Epoch 8/10\n",
      "19286/19286 [==============================] - 70s - loss: 0.0051 - val_loss: 0.0143\n",
      "Epoch 9/10\n",
      "19286/19286 [==============================] - 70s - loss: 0.0049 - val_loss: 0.0136\n",
      "Epoch 10/10\n",
      "19286/19286 [==============================] - 68s - loss: 0.0046 - val_loss: 0.0157\n"
     ]
    }
   ],
   "source": [
    "lenet = Sequential()\n",
    "lenet.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160, 320, 3)))\n",
    "lenet.add(Convolution2D(6, 5, 5, activation='relu'))\n",
    "lenet.add(MaxPooling2D())\n",
    "lenet.add(Convolution2D(6, 5, 5, activation='relu'))\n",
    "lenet.add(MaxPooling2D())\n",
    "lenet.add(Flatten())\n",
    "lenet.add(Dense(120))\n",
    "lenet.add(Dense(84))\n",
    "lenet.add(Dense(1))\n",
    "\n",
    "\n",
    "lenet.compile(loss='mse', optimizer='adam')\n",
    "lenet.fit(X_train_d, y_train_d, validation_split=0.2, shuffle=True)\n",
    "\n",
    "lenet.save('lenet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flipping Images Vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_aug, y_aug = [], []\n",
    "\n",
    "for X, y in zip(X_train_d, y_train_d):\n",
    "    X_aug.append(X)\n",
    "    y_aug.append(y)\n",
    "    X_aug.append(cv2.flip(X, 1))\n",
    "    y_aug.append(y * -1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-40a5749a3c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_aug, y_aug = np.array(X_aug), np.array(y_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43790, 160, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "aug_data = {\"features\" : X_aug,\"labels\" : y_aug}\n",
    "pickle.dump(aug_data, open( \"train_aug.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19286 samples, validate on 4822 samples\n",
      "Epoch 1/10\n",
      "19286/19286 [==============================] - 49s - loss: 0.0682 - val_loss: 0.0214\n",
      "Epoch 2/10\n",
      "19286/19286 [==============================] - 46s - loss: 0.0142 - val_loss: 0.0208\n",
      "Epoch 3/10\n",
      "19286/19286 [==============================] - 46s - loss: 0.0119 - val_loss: 0.0187\n",
      "Epoch 4/10\n",
      "19286/19286 [==============================] - 46s - loss: 0.0106 - val_loss: 0.0180\n",
      "Epoch 5/10\n",
      "19286/19286 [==============================] - 46s - loss: 0.0098 - val_loss: 0.0195\n",
      "Epoch 6/10\n",
      "19286/19286 [==============================] - 46s - loss: 0.0090 - val_loss: 0.0175\n",
      "Epoch 7/10\n",
      "19286/19286 [==============================] - 46s - loss: 0.0086 - val_loss: 0.0184\n",
      "Epoch 8/10\n",
      "19286/19286 [==============================] - 47s - loss: 0.0083 - val_loss: 0.0184\n",
      "Epoch 9/10\n",
      "19286/19286 [==============================] - 46s - loss: 0.0081 - val_loss: 0.0190\n",
      "Epoch 10/10\n",
      "19286/19286 [==============================] - 46s - loss: 0.0077 - val_loss: 0.0187\n"
     ]
    }
   ],
   "source": [
    "lenet_aug = Sequential()\n",
    "lenet_aug.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(160,320,3)))\n",
    "lenet_aug.add(Lambda(lambda x: x / 255.0 - 0.5))\n",
    "lenet_aug.add(Convolution2D(6, 5, 5, activation='relu'))\n",
    "lenet_aug.add(MaxPooling2D())\n",
    "lenet_aug.add(Convolution2D(6, 5, 5, activation='relu'))\n",
    "lenet_aug.add(MaxPooling2D())\n",
    "lenet_aug.add(Flatten())\n",
    "lenet_aug.add(Dense(120))\n",
    "lenet_aug.add(Dense(84))\n",
    "lenet_aug.add(Dense(1))\n",
    "\n",
    "lenet_aug.compile(loss='mse', optimizer='adam')\n",
    "lenet_aug.fit(X_train_d, y_train_d, validation_split=0.2, shuffle=True)\n",
    "\n",
    "lenet_aug.save('lenet_default_train.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvidia End to End Model\n",
    "\n",
    "<img src=\"https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/08/cnn-architecture.png\" alt=\"Nvidia Network\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19286 samples, validate on 4822 samples\n",
      "Epoch 1/5\n",
      "19200/19286 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.1804Epoch 00000: val_loss improved from -inf to 0.04424, saving model to nividia-improvement-00-0.18.hdf5\n",
      "19286/19286 [==============================] - 69s - loss: 0.0447 - acc: 0.1799 - val_loss: 0.0442 - val_acc: 0.1837\n",
      "Epoch 2/5\n",
      "19200/19286 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.1795Epoch 00001: val_loss did not improve\n",
      "19286/19286 [==============================] - 41s - loss: 0.0356 - acc: 0.1798 - val_loss: 0.0317 - val_acc: 0.1837\n",
      "Epoch 3/5\n",
      "19200/19286 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.1798Epoch 00002: val_loss did not improve\n",
      "19286/19286 [==============================] - 41s - loss: 0.0284 - acc: 0.1800 - val_loss: 0.0310 - val_acc: 0.1837\n",
      "Epoch 4/5\n",
      "19200/19286 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.1799Epoch 00003: val_loss did not improve\n",
      "19286/19286 [==============================] - 41s - loss: 0.0243 - acc: 0.1799 - val_loss: 0.0237 - val_acc: 0.1837\n",
      "Epoch 5/5\n",
      "19200/19286 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.1801Epoch 00004: val_loss did not improve\n",
      "19286/19286 [==============================] - 41s - loss: 0.0226 - acc: 0.1801 - val_loss: 0.0213 - val_acc: 0.1837\n"
     ]
    }
   ],
   "source": [
    "nvidia = Sequential()\n",
    "nvidia.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "nvidia.add(Cropping2D(cropping=((70,25), (0,0))))\n",
    "nvidia.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "nvidia.add(Dropout(0.5))\n",
    "nvidia.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "nvidia.add(Dropout(0.5))\n",
    "nvidia.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "nvidia.add(Dropout(0.5))\n",
    "nvidia.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "nvidia.add(Dropout(0.5))\n",
    "nvidia.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "nvidia.add(Dropout(0.5))\n",
    "nvidia.add(Flatten())\n",
    "nvidia.add(Dense(100))\n",
    "nvidia.add(Dropout(0.5))\n",
    "nvidia.add(Dense(50))\n",
    "nvidia.add(Dropout(0.5))\n",
    "nvidia.add(Dense(10))\n",
    "nvidia.add(Dense(1))\n",
    "\n",
    "# Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "nvidia.compile(loss='mse', optimizer=opt,  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"nividia-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = nvidia.fit(X_train_d, y_train_d, validation_split=0.2, batch_size=128, nb_epoch=5, callbacks=callbacks_list, shuffle=True)\n",
    "\n",
    "nvidia.save('nvidia_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl4VOXZ+PHvnZAQ9hBCWBJC2JOwhRAWN2QTAVEq4oK2\nKi4otVq1tT/1rVW78rbu1tcFRItVqEWtlIIrIFJFIOwQdgMkBMIadshy//44BxxCVpLJmST357rm\ncs6c55y55xDnnmc5zyOqijHGGHOhgrwOwBhjTPVmicQYY0yFWCIxxhhTIZZIjDHGVIglEmOMMRVi\nicQYY0yFWCIxphgiEiciKiJ1ylD2dhFZVBVxGRNoLJGYGkFE0kXktIhEFnp9hZsM4ryJ7JyEtKLQ\n65FuzOk+r10qIt+ISI6IHBCR/4pIH3ff7SKSLyJHCz1aF/O+KiId/frhjMESialZvgfGndkQke5A\nfe/COU99Eenms30zTswAiEhjYDbwMhABRANPA6d8jvlWVRsWeuyqgtiNKZYlElOTvAPc6rN9GzDN\nt4CINBGRaSKyV0S2i8ivRSTI3RcsIs+IyD4R2QZcVcSxb4pIlohkisjvRSS4nPHd5rN9a6H4OgOo\n6nRVzVfVE6r6maquLsd7lEpEgtzPvV1Est3r0cTdFyYifxeR/SJySESWikgLd9/tIrJNRI6IyPci\ncktlxmWqL0skpiZZDDQWkQT3C/4m4O+FyrwMNAHaA5fjfJmPd/fdDYwCegEpwNhCx74N5AEd3TLD\ngLvKEd/fgZvchJUINAS+89m/CcgXkb+JyAgRaVqOc5fH7e5jEM51aAj81d13G871aQM0A+4FTohI\nA+AlYISqNgIuBlb6KT5TzVgiMTXNmVrJFUAakHlmh09yeUxVj6hqOvAs8BO3yA3AC6q6U1UPAH/y\nObYFMBJ4UFWPqWo28Lx7vrLKADYCQ90Y3/HdqaqHgUsBBSYDe0Vk1pkagau/W1M489hajvc/4xbg\nOVXdpqpHgcdwElwdIBcngXR0a0WpblwABUA3Eamnqlmquu4C3tvUQJZITE3zDk7fw+0UatYCIoEQ\nYLvPa9tx+iIAWgM7C+07o617bNaZL3HgdSCqnPFNc2MbR6FEAqCqaap6u6rGAN3cmF7wKbJYVcN9\nHh3K+f645yx8DeoALdyYPgVmiMguEfmziISo6jHgRpwaSpaI/EdE4i/gvU0NZInE1Ciquh2nA3sk\n8GGh3ftwfnG39Xktlh9qLVk4TTq++87YidPpHenzJd5YVbuWM8QPcPpetqnqjlI+ywac5rRuJZW7\nALs4/xrkAXtUNVdVn1bVRJzmq1G4/U6q+qmqXgG0Ajbg1JqMsURiaqQ7gcHur+izVDUfeB/4g4g0\nEpG2wMP80I/yPvCAiMS4/ROP+hybBXwGPCsijd0O6w4icnl5AnNjGkwRfSsiEi8ivxCRGHe7DU7N\nZXF53qOQULcD/cwjGJgOPCQi7USkIfBH4B+qmicig0Sku1vuME7iLRCRFiIy2u0rOQUcxWnqMsYS\nial5VHWrqi4rZvf9wDFgG7AIeA+Y6u6bjNOsswpYzvk1mluBUGA9cBCYifPrvLzxLVPVovo2jgD9\ngO9E5BhOAlkL/MKnzEVF3EfSp4S3Wwec8HmMx/m87wALcWpvJ3GuC0BL93Mdxulj+sotG4STdHcB\nB3AGKkws72c3NZPYwlbGGGMqwmokxhhjKsQSiTHGmAqxRGKMMaZCLJEYY4ypkFKnx64JIiMjNS4u\nzuswjDGmWklNTd2nqs1LK1crEklcXBzLlhU3GtQYY0xRRGR76aWsacsYY0wFWSIxxhhTIZZIjDHG\nVEit6CMxxngnNzeXjIwMTp486XUophhhYWHExMQQEhJyQcdbIjHG+FVGRgaNGjUiLi4OEfE6HFOI\nqrJ//34yMjJo167dBZ3DmraMMX518uRJmjVrZkkkQIkIzZo1q1CN0RKJMcbvLIkEtor++1giKcGS\n7w/w6oILWcnUGGNqD0skJfh03W7+/OkG1mbmeB2KMeYC7d+/n6SkJJKSkmjZsiXR0dFnt0+fPl2m\nc4wfP56NGzeWWOaVV17h3XffrYyQ+fjjj0lKSqJnz54kJiYyZcqUEsvPmzePxYsrsv5Zxfi1s11E\nhgMvAsHAFFWdVGi/uPtHAseB21V1ubsy3DScNaQVeENVX3SPiQD+AcQB6cANqnrQH/H/fGgnPl6Z\nyZOz1jHz3ousem5MNdSsWTNWrlwJwFNPPUXDhg355S9/eU4ZVUVVCQoq+rf1W2+9Ver73HfffRUP\nFjh16hQTJ05k2bJltG7dmlOnTrF9e8k3mM+bN4/IyEj69+9fKTGUl99qJO5Sna8AI4BEYJyIJBYq\nNgLo5D4mAK+6r+cBv3DXje4P3Odz7KPAl6raCfgSn+VQK1vjsBB+NTye1O0H+dfKzNIPMMZUG1u2\nbCExMZFbbrmFrl27kpWVxYQJE0hJSaFr16789re/PVv20ksvZeXKleTl5REeHs6jjz5Kz549ueii\ni8jOzgbg17/+NS+88MLZ8o8++ih9+/alS5cufPPNNwAcO3aM6667jsTERMaOHUtKSsrZJHdGTk4O\nqkpERAQAdevWpXPnzgDs2bOHMWPGkJKSQt++fVm8eDFbt25lypQp/OUvfyEpKense1Ulf9ZI+gJb\nVHUbgIjMAEbjLFN6xmhgmjrLNC4WkXARaeWuj50FoKpHRCQNiHaPHQ0MdI//G7AA+H/++hBjk2N4\nd/F2/jRnA1cktqRhXRsxbcyFevrf61i/63ClnjOxdWOevLrrBR27YcMGpk2bRkpKCgCTJk0iIiKC\nvLw8Bg0axNixY0lMPPf3b05ODpdffjmTJk3i4YcfZurUqTz66Pm/Z1WVJUuWMGvWLH7729/yySef\n8PLLL9OyZUs++OADVq1aRXJy8nnHRUVFceWVV9K2bVuGDBnC1VdfzY033khQUBAPPPAAv/rVr+jf\nvz/p6emMGjWKtWvXctdddxEZGcmDDz54QdehovzZRxIN7PTZznBfK1cZEYkDegHfuS+1cBMNwG6c\n5q/ziMgEEVkmIsv27t17IfEDEBQkPHVNV7KPnOKv87Zc8HmMMYGnQ4cOZ5MIwPTp00lOTiY5OZm0\ntDTWr19/3jH16tVjxIgRAPTu3Zv09PQizz1mzJjzyixatIibbroJgJ49e9K1a9EJ8O233+bzzz8n\nJSWFSZMmMWHCBAC++OIL7r33XpKSkvjRj37EwYMHOXHixAV99soU0D+vRaQh8AHwoKqe9zNGVVVE\nilx0XlXfAN4ASElJqdDC9L1imzK2dwxvLtrGDSkxtG/esCKnM6bWutCag780aNDg7PPNmzfz4osv\nsmTJEsLDw/nxj39c5L0VoaGhZ58HBweTl5dX5Lnr1q1bapmS9OjRgx49enDzzTeTkJDAlClTztZy\nfGMIBP6skWQCbXy2Y9zXylRGREJwksi7qvqhT5k9ItLKLdMKyK7kuIv0/4bHE1YnmN/NPv8XijGm\n+jt8+DCNGjWicePGZGVl8emnn1b6e1xyySW8//77AKxZs6bIGs/hw4dZuHDh2e2VK1fStm1bAIYO\nHcorr7xyzj6ARo0aceTIkUqPt6z8mUiWAp1EpJ2IhAI3AbMKlZkF3CqO/kCOqma5o7neBNJU9bki\njrnNfX4b8LH/PsIPmjeqy8+HdmL+xr3M27CnKt7SGFOFkpOTSUxMJD4+nltvvZVLLrmk0t/j/vvv\nJzMzk8TERJ5++mkSExNp0qTJOWVUlT/96U906dKFpKQkfv/73zN16lTAGWL83//+lx49epCYmMjk\nyZMBGD16NO+//z69evXypLNdnH5uP51cZCTwAs7w36mq+gcRuRdAVV9zE8ZfgeE4w3/Hq+oyEbkU\n+BpYAxS4p3tcVeeISDPgfSAW2I4z/PdASXGkpKRoZSxsdTqvgBEvLiS/QPn0oQHUrRNc4XMaU9Ol\npaWRkJDgdRgBIS8vj7y8PMLCwti8eTPDhg1j8+bN1KnjfS9DUf9OIpKqqinFHHKWX6NX1TnAnEKv\nvebzXIHzBl+r6iKgyJs2VHU/MKRyIy2b0DpBPHVNV37y5hLeXPQ9Px3Y0YswjDHV1NGjRxkyZAh5\neXmoKq+//npAJJGKqv6foIpd1qk5wxJb8Nd5WxjTK4aWTcK8DskYU02Eh4eTmprqdRiVzqZIuQC/\nviqRvAJl0tw0r0MxxhjPWSK5ALHN6nPPgPb8a+UulqaX2D1jjDE1niWSC/TTgR1p3SSMJz9eR36B\n/wYsGGNMoLNEcoHqhQbz+FUJrM86zIylO7wOxxhjPGOJpAKu6t6K/u0jeObTjRw6XrbpqI0xVWvQ\noEHn3Vz4wgsvMHHixBKPa9jQmcFi165djB07tsgyAwcOpLRbC1544QWOHz9+dnvkyJEcOnSoLKGX\naOPGjQwcOJCkpCQSEhLOTqNSnPT0dN57770Kv29RLJFUgIgzD1fOiVye+3yT1+EYY4owbtw4ZsyY\ncc5rM2bMYNy4cWU6vnXr1sycOfOC379wIpkzZw7h4eEXfL4zHnjgAR566CFWrlxJWloa999/f4nl\nLZEEsPiWjflJ/7b8ffF20rIqd1ZTY0zFjR07lv/85z9nF7FKT09n165dXHbZZWfv60hOTqZ79+58\n/PH5E2Wkp6fTrVs3AE6cOMFNN91EQkIC11577TkTJk6cOPHsFPRPPvkkAC+99BK7du1i0KBBDBo0\nCIC4uDj27dsHwHPPPUe3bt3o1q3b2Sno09PTSUhI4O6776Zr164MGzasyIkZs7KyiImJObvdvXt3\nAPLz83nkkUfo06cPPXr04PXXXwfg0Ucf5euvvyYpKYnnn3++Yhe1ELuPpBI8dEVnZq3axVOz1jFj\nQn9bAMuY4sx9FHavqdxztuwOIyYVuzsiIoK+ffsyd+5cRo8ezYwZM7jhhhsQEcLCwvjoo49o3Lgx\n+/bto3///lxzzTXF/j/86quvUr9+fdLS0li9evU508D/4Q9/ICIigvz8fIYMGcLq1at54IEHeO65\n55g/fz6RkZHnnCs1NZW33nqL7777DlWlX79+XH755TRt2pTNmzczffp0Jk+ezA033MAHH3zAj3/8\n43OOf+ihhxg8eDAXX3wxw4YNY/z48YSHh/Pmm2/SpEkTli5dyqlTp7jkkksYNmwYkyZN4plnnmH2\n7NkVuNhFsxpJJQivH8ojV8bz3fcHmL06q/QDjDFVyrd5y7dZS1V5/PHH6dGjB0OHDiUzM5M9e4qf\nS2/hwoVnv9DPzM57xvvvv09ycjK9evVi3bp1RU7I6GvRokVce+21NGjQgIYNGzJmzBi+/vprANq1\na0dSUhJQ/FT148ePJy0tjeuvv54FCxbQv39/Tp06xWeffca0adNISkqiX79+7N+/n82bN5f9Yl0A\nq5FUkhv7tOHd77bzxzlpDEmIon6oXVpjzlNCzcGfRo8ezUMPPcTy5cs5fvw4vXv3BuDdd99l7969\npKamEhISQlxcXJFTx5fm+++/55lnnmHp0qU0bdqU22+//YLOc8aZKejBmYa+uDVHWrduzR133MEd\nd9xBt27dWLt2LarKyy+/zJVXXnlO2QULFlxwPKWxGkklCQ4Snr6mK1k5J3l1wVavwzHG+GjYsCGD\nBg3ijjvuOKeTPScnh6ioKEJCQpg/f36pa6MPGDDgbIf12rVrWb16NeBM/d6gQQOaNGnCnj17mDt3\n7tljipvi/bLLLuNf//oXx48f59ixY3z00UdcdtllZf5Mn3zyCbm5uQDs3r2b/fv3Ex0dzZVXXsmr\nr756dt+mTZs4duyYX6eat0RSiVLiIvhRUmteX7iNHfuPl36AMabKjBs3jlWrVp2TSG655RaWLVtG\n9+7dmTZtGvHx8SWeY+LEiRw9epSEhAR+85vfnK3Z9OzZk169ehEfH8/NN998zhT0EyZMYPjw4Wc7\n289ITk7m9ttvp2/fvvTr14+77rqLXr16lfnzfPbZZ3Tr1o2ePXty5ZVX8pe//IWWLVty1113kZiY\nSHJyMt26deOee+4hLy+PHj16EBwcTM+ePSu9s92v08gHisqaRr4s9hw+yaBnFnBJx0gm31rq7MvG\n1Hg2jXz1UJFp5K1GUslaNA7j/sGd+Hz9Hr7adOFrxRtjTHVhicQP7rg0jnaRDXj63+s4nVdQ+gHG\nGFONWSLxg7p1gvnNqES27T3G375J9zocYzxXG5rQq7OK/vtYIvGTQfFRDI6P4sUvN5N95MKHARpT\n3YWFhbF//35LJgFKVdm/fz9hYRe+SJ/d7OBHT4xKZNjzX/HnTzbyzPU9vQ7HGE/ExMSQkZHB3r3W\nZxiowsLCzplupbz8mkhEZDjwIhAMTFHVSYX2i7t/JHAcuF1Vl7v7pgKjgGxV7eZzTE/gNaAhkA7c\noqoBOclVu8gG3Hlpe177ais394slObap1yEZU+VCQkJo166d12EYP/Jb05aIBAOvACOARGCciCQW\nKjYC6OQ+JgCv+ux7GxhexKmnAI+qanfgI+CRyo28ct0/uCMtGtflqVnrKLAFsIwxNZA/+0j6AltU\ndZuqngZmAKMLlRkNTFPHYiBcRFoBqOpCoKh1bDsDC93nnwPX+SX6StKgbh0eG5HA6owcZqZmeB2O\nMcZUOn8mkmhgp892hvtaecsUto4fEtL1QJuiConIBBFZJiLLvG6bHZ3UmpS2TfnfTzaQcyLX01iM\nMaayVcdRW3cAPxWRVKARUOTShKr6hqqmqGpK8+bNqzTAws4sgHXg+Gle/MK/s3AaY0xV82ciyeTc\n2kKM+1p5y5xDVTeo6jBV7Q1MB6rFDIndopswrm8sf/s2nc17/DNxmjHGeMGfo7aWAp1EpB1OcrgJ\nuLlQmVnAz0RkBtAPyFHVEhf0EJEoVc0WkSDg1zgjuPxj21fuIjwKqqAFPzw/57+UoYzyRFgeHUPT\n2fjOh3Ts3hLB7Xw/r2xBkcef/x4UU6aU48+epyzvUVQZin+PsMbQYQh0GQERNlLHmNrAb4lEVfNE\n5GfApzjDf6eq6joRudfd/xowB2fo7xac4b/jzxwvItOBgUCkiGQAT6rqmzijv+5zi30IvOWvz8D6\nj2HZmxU4gYDI2f/WkyBuC4LcI0rBkmCCg4IKlQkCOf+488sUtw9nf7mP93nPUo/3KRPkVmh99x3a\nAZ8+5jyaxzsJpfMIiEmBoOAKXEtjTKCy2X9LknsC8k9T+hd4oS/pEpbazcsvYNTLizhyMo8vf3E5\nYSE18Mv1wDbY+Alsmgvp/wXNh/qR0PlK6DwcOgyGug29jtIYU4qyzv5ricQD327dz7jJi3loaGd+\nPrST1+H414lDsOUL2DgXtnwOJ3MgOBTaDfihttKktIF6xhgvWCLxEWiJBOC+95bzxfo9fPmLy4lp\nWt/rcKpGfi7s+NaprWycAwe/d15v2cNNKsOhVdIPTWbGGE9ZIvERiIlk16ETDH52AYPjo/i/W3p7\nHU7VU4V9m5yaysa5kLHE6cBv1MppAusy0qm1hNTzOlJjaq2yJhKbtNEjrcPrcd/Ajjz7+Sa+2bKP\niztGeh1S1RKB5l2cx6UPwrH9sPkzp6ayZiakvg0h9aH9IOgyHDpdCY1aeB21MaYIViPx0MncfK54\n/ivqhQQz54HLqBNsTToA5J2C9K/dJrC5cNidWiY6xUkqXUZCVGKJgxqMMRVnTVs+AjWRAHy6bjf3\nvJPKk1cnMv4Su+/iPKqwZ+0P/Sq7ljuvN4l1+lW6DIe2l0KdUG/jNKYGskTiI5ATiapy69QlrNx5\niAW/HEizhnW9DimwHdkNmz5xEsu2+ZB3EkIbQUf3JshOw6B+hNdRGlMjWCLxEciJBGBL9hGGv/A1\n16fE8KcxPbwOp/o4fRy+/8pp/tr0CRzd49zL06b/D01gkTV8eLUxfmSJxEegJxKA389ez5v//Z6P\n77uEHjHhXodT/RQUQNYKdxTYJ7BnjfN6RAe3CWyEk2CCbXyJMWVlicRHdUgkR07mMuiZr4iNqMfM\ney8mKMg6kivk0A7Y9KmTWL5fCAW5EBbuNH11GQ4dh0JYE6+jNCagWSLxUR0SCcA/l+3kkZmree6G\nnoxJvvD1k00hp47A1nluE9incOIABNWBtpc4zV9dhkPTOK+jNCbgWCLxUV0SSUGBMubVb8g8dIL5\nvxxIw7rWDFPpCvIhY6kzAmzjJ7Bvo/N6VKJzZ32XkRDd2+6uNwZLJOeoLokEYNXOQ4x+5b/cM6A9\nj41M8Dqcmm//VncU2FzY/o0zwWSD5s4NkF1GQIdBENrA6yiN8YQlEh/VKZEA/GrmKj5akcknDw6g\nQ3ObJbfKnDgIW750aiubv4BTORBcF9pf7tRWOg+3CSZNrWKJxEd1SyR7j5xi8DMLSG7blLfH90Hs\nDu6ql5/r1FA2nZlgMt15vVVPZ8biLiOc5/ZvY2owSyQ+qlsiAZjy9TZ+/580ptyawtBEm2PKU6qw\nd6OzvsrGubBzCaDQqLXTUd95hDvBZJjXkRpTqSyR+KiOiSQ3v4ARL37N6bwCPntoQM1cAKu6OrrX\nmWBy01zYMg9yjzkTTHYY7DaBXQkNo7yO0pgKs0TiozomEoCvN+/lJ28u4ZEru3DfoI5eh2OKknsS\n0hf9UFs5nAmIs7TwmVFgUQnWBGaqJUskPqprIgG4551lLNy0j3m/vJxWTWxtjoCmCrvXuPerzIVd\nK5zXw2OdhNJ5uHPvik0waapKQUGFhrJbIvFRnRPJzgPHGfrcV1zZtSUvjevldTimPA5nOZ31mz6B\nbQucCSbrNnYmmOw8AjpdYRNMGv84fgCW/w2Wvgk3/wNadL2g05Q1kfj1risRGS4iG0Vki4g8WsR+\nEZGX3P2rRSTZZ99UEckWkbWFjkkSkcUislJElolIX39+Bq+1iajPPZd3YNaqXSz5/oDX4ZjyaNwK\nUsY7/yP/6nu4aTp0/RGk/xc+mgB/6Qhvj3IW8so77XW0pibYsx5mPQDPJcIXTzkzNuT7/2/LbzUS\nEQkGNgFXABnAUmCcqq73KTMSuB8YCfQDXlTVfu6+AcBRYJqqdvM55jPgeVWd6x7/K1UdWFIs1blG\nAnDidD5Dnl1Ak/qhzL7/UoJtHq7qraDAafbaOAfWfuCsXd8gCnrfBr3H270qpnwK8p2pf7571ZlX\nrk4Y9LgB+t4DLbuVfnwJAqFG0hfYoqrbVPU0MAMYXajMaJxEoaq6GAgXkVYAqroQKOonuAKN3edN\ngF1+iT6A1AsN5n+uSiQt6zDvLdnhdTimooKCIKY3DHkC7l8Ot3wA0cmw8Bl4oTv848dOU1gtaHY2\nFXDiEHz7CrycDDPGObM0DHkSHloP17xc4SRSHv6czCka2OmznYFT6yitTDSQVcJ5HwQ+FZFncBLh\nxUUVEpEJwASA2NjYcgUeiEZ2b8lF7Zvx7GcbGdW9FU0bWIdtjRAUBJ2GOo+D6bBsKix/B9L+DZGd\noc9d0HMchDUu9VSmlti3Gb57HVa+5ww9b9Mfhj4F8aMgOMSTkKrjzHQTgYdUtQ3wEPBmUYVU9Q1V\nTVHVlObNm1dpgP4gIjx5TSJHTubx3OebvA7H+EPTOLjit/BwGvzoNajbCOb+Cp6Nh9kPOe3fpnYq\nKIDNn8Pfr4O/pjgd6YnXwIQFcOen0PVaz5II+LdGkgm08dmOcV8rb5nCbgN+7j7/JzClAjFWK/Et\nG/OT/m2Z9m064/rGktjafqXWSCFhkDTOeWQuh6VTYMW7Tm2l7SVOLSXhak+/OEwVOXUEVk6HJa/D\n/i3QsAUMfNwZxBFAN736s0ayFOgkIu1EJBS4CZhVqMws4FZ39FZ/IEdVS2rWAqdP5HL3+WBgc2UG\nHegeGtqZ8PqhPDVrHbVh6HatF50MP/o/+MUGp7aSkwEzx8PzXWH+H+Fwje8irJ0ObINPHnNGX819\nxBk2PmYyPLgWBv6/gEoi4Of7SNxRVS8AwcBUVf2DiNwLoKqviTMb4V+B4cBxYLyqLnOPnQ4MBCKB\nPcCTqvqmiFwKvIhTmzoJ/FRVU0uKo7qP2ips+pIdPPbhGl4a14trerb2OhxTlQryYcsXsGSy818J\ngoRR0OduiLvU7qCvzlTh+69g8WvOvUdBwZD4I+g/0ZkpwQN2Q6KPmpZI8guU0a8sYt+R08z75eXU\nD7UFsGqlA9t+6Jw/eQiaJ0CfO6HnTU7/iqkeTh+H1f9wOtD3pkH9SKfpKuVO514kD1ki8VHTEglA\n6vYDXPfqt9w3qAOPXBnvdTjGS7knnPtRlkyGrJUQ2shJJn3ugij72whYh3Y4/V+pf3N+CLTsDv0m\nQrfrAmYmaUskPmpiIgF4+B8rmb06i88fHkDbZraKX62nCpmpTkJZ96FzR3PcZU5Cib/KOucDgaqz\nzs13r8GG2c5r8aOc5qvYiwKuadISiY+amkj2HD7J4GcWcFGHSKbc5k0bqglQx/bBindg6VTI2QGN\nWjl3zfe+DRq19Dq62if3JKyd6SSQ3WsgLNz5t+hzN4S3Kf14j1gi8VFTEwnAa19tZdLcDbw9vg8D\nuwTWSA4TAArynbVTlkyGrV9CUB1IuMappbS9OOB+Adc4h7Pc5qu34Ph+px+r3z3Q40YIre91dKWy\nROKjJieS03kFDH9hIQCfPDiA0DrV8R5TUyX2b3Vmg135dziZA1Fdnc75HjdC3YZeR1ez7FzqzH21\n/mMnmXcZ4SSQdpdXq+RticRHTU4kAPM3ZjP+raU8PjKeCQM6eB2OCXSnj8Oaf8LSyU4zS93GzjQs\nfe6C5p29jq76yjsN6//lNF9lpjrXtddPoO9dENHe6+guiCUSHzU9kQDc+fZSFm/bz/xfDiSqcWCM\n+DABTtVZf37pFFj3ERTkOr+Y+97trJcSbMPKy+RoNix7C5a9CUf3QLOO0O/eGjEM2xKJj9qQSNL3\nHWPY8wsZ1bMVz92Q5HU4pro5uteZv2nZW3A4AxrHQMrtkHxbwN1FHTB2rXRqH2s/cEbIdbzCSSAd\nBldoVcJAYonER21IJAB//mQD/7dgKx9MvJjebZt6HY6pjvLznLuql052prIPCoHE0U4tpU2/atW+\n7xf5uc60v7G1AAAgAElEQVTMzN+9DjsXQ0gD6HUL9J0AkZ28jq7SWSLxUVsSybFTeQx+dgFRjcL4\n+L5LCLIFsExF7Nvsds6/B6dyoEV3p72/+/UQWsvuWzq2H5a/7VyPw5nOTM1973GSSFgTr6PzG0sk\nPmpLIgH4eGUmP5+xkkljunNT3+q/DosJAKePwer3nb6UPWuhbhPnCzTlTojs6HV0/rV7rdN8teaf\nkHfS6UPqPxE6DXPmwqrhKi2RiMj1wCeqekREfg0kA79X1eWVE6r/1aZEoqrc8Pq3bN17jPm/HEiT\nenY3s6kkqrBjsdPstf5jKMiD9oPczvnhNeeLtSAfNs51Ekj611CnHvS80amBtEj0OroqVZmJZLWq\n9nBn3f098BfgN2fWVq8OalMiAVi3K4erX17EbRfH8eTVXb0Ox9RER/b80Dl/ZBc0aeNMNJh8GzSI\n9Dq6C3PiIKz4Oyx5w5kHq3GMkySTb4X6EV5H54nKTCQrVLWXiPwJWKOq7515rbKC9bfalkgA/uej\nNcxYupM5D1xGl5bVewiiCWD5ebBxjlNL+X4hBIc6q/X1uduZ+rw6dM7v3eh0nq+aDrnHIfZi6H8v\ndLmq1g+BrsxEMhtn1cIrcJq1TgBLVLVnZQRaFWpjIjl47DQDn1lA19aNefeufkh1+B/aVG97Nzr9\nKCunw+kj0LKH84u+29jAmw6koAC2fO40X22d5yTA7tc7d5+3qjZfbX5XmYmkPs7CU2tUdbOItAK6\nq+pnlROq/9XGRALwzrfpPPHxOl69JZkR3b1d18DUIqeOOOtrLJnirK8RFg69fgwpd0Azj2deOHnY\nGYW25HVnPZdGrZxpYpJvh4bNvY0tAFVmIukAZKjqKREZCPQApqnqoUqJtArU1kSSl1/AqJcXceRk\nHl88fDn1QmtIZ6ipHlRh+3+dCSM3zHY65zsOdZq9Ol1RtZ3z+7c6fR8r3nVqSzF9nJsHE0fb9Pol\nqMxEshJIAeKAOcDHQFdVHVkJcVaJ2ppIABZv289Nbyzm50M68dAVNo+S8cjhrB8654/uhvBYZ/hw\nr59Ag2b+eU9V2Dbf6f/Y9Kkz83HXa50EEtPbP+9Zw1RmIlmuqski8ivghKq+bJ3t1cv901fw2brd\nfPHw5bSJCLC2alO75Oc6tZMlU2D7IgiuC93GuJ3zlfTlfvoYrJrhJJB9G6FBc6dZLeUOW4ulnMqa\nSMoyIUyuiIwDbgXcJb0oU11QRIaLyEYR2SIijxaxX0TkJXf/ahFJ9tk3VUSyRWRtoWP+ISIr3Ue6\nW2MyJXhsRDxBIvxxTprXoZjaLjjEqRWM/w9M/NbpO0n7N0wZDG8MdJqeck9c2LkPbofPfg3PJcB/\nHnaWq/3Ra/DQOhj0uCURPypLjSQRuBf4VlWni0g74AZV/d9SjgsGNuGM9soAlgLjVHW9T5mRwP3A\nSKAf8OKZ+1NEZABwFKc/plsx7/EskKOqvy0pltpeIwH467zNPPPZJt69qx+XdKym4/xNzXTysNs5\nP9mpQdRr6jR59bnTmYqkJGf6YRa/6gxDRiDxGqf5yuYGq7BKnSJFREKBMw3sG1U1twzHXAQ8papX\nutuPAajqn3zKvA4sUNXp7vZGYKCqZrnbccDsohKJOONZdwCDVXVzSbFYIoGTufkMe34hdesEMefn\nlxESXDNmJzU1iKpzJ/mSybDhP6AFTqd8n7udTnrfGXVzT8CamU7z1Z41UC8Cet/uJJ8mMZ59hJqm\nrImk1Ltt3JFafwPSAQHaiMhtqrqwlEOjgZ0+2xk4tY7SykQDWaXFBVwG7CkuiYjIBGACQGyszTkV\nFhLME6MSuXvaMt75djt3XNrO65CMOZcItBvgPHIyIfVt5/He9U7NJOVOJ7Gs+afTaX/igLPK4zUv\nO/eAhNTz+APUXmW5bfNZYJiqbgQQkc7AdMDrYQ/j3DiKpKpvAG+AUyOpqqAC2dCEKAZ0bs7zX2zi\nmqTWRDas63VIxhStSTQM/h8Y8AikzXJudPz8CeeBQPxVzs2DcZdZ81UAKEsiCTmTRABUdZOIlKWz\nPRNo47Md475W3jLnEZE6wBi8T2bViojwm1GJDH9hIX/5ZCP/O7aH1yEZU7I6odB9rPPYvRbSF0GX\n4aX3nZgqVZaG8mUiMkVEBrqPyUBZOhyWAp1EpJ3bx3ITMKtQmVnAre7orf44HedladYaCmxQ1Ywy\nlDU+OkY15I5L2/F+6k5W7aw295QaAy27OXNgWRIJOGVJJBOB9cAD7mM9ziiuEqlqHvAz4FMgDXhf\nVdeJyL0icub4OcA2YAswGfjpmeNFZDrwLdBFRDJE5E6f099ECc1apmT3D+5IswZ1eerf6ygosFY/\nY0zFXNDCViLyD1W90Q/x+IWN2jrfzNQMfvnPVTxzfU/G9rZRLsaY81XmDYlFuegCjzMBYkyvaHrF\nhjNp7gaOnCx1NLcxxhTLbiaopYKChKeu7sr+Y6d4ed4Wr8MxxlRjxY7a8p2upPAuyjhFiglsPduE\nc0PvNkxd9D03pLShY1RDr0MyxlRDJQ3/fbaEfRsqOxDjjUeGd2HO2ix+O3s9fxvfxxbAMsaUW7GJ\nRFUHVWUgxhuRDevy0NDO/Hb2er5Iy+aKxBZeh2SMqWasj8Twk4va0imqIb+bvZ6Tufleh2OMqWYs\nkRhCgoN46pqu7DhwnClfb/M6HGNMNWOJxABwScdIRnRrySvzt7Lr0AWuB2GMqZWKTSQi8mOf55cU\n2vczfwZlvPH4yAQKVPnTXBtLYYwpu5JqJA/7PH+50L47/BCL8VibiPrce3kH/r1qF4u37fc6HGNM\nNVFSIpFinhe1bWqIey/vQHR4PZ6atY68/AKvwzHGVAMlJRIt5nlR26aGqBcazK+vSmDD7iNMX7LD\n63CMMdVASTckxovIapzaRwf3Oe52e79HZjwzvFtLLu7QjGc+28SoHq1p2iDU65CMMQGspBpJAnA1\nMMrn+ZntRP+HZrwiIjx1TVeOnsrjmc82ln6AMaZWKzaRqOp23wdwFEgGIt1tU4N1btGIWy9qy3tL\ndrA2M8frcIwxAayk4b+zRaSb+7wVsBZntNY7IvJgFcVnPPTg0M40rR/K0/9ex4WsW2OMqR1Katpq\np6pr3efjgc9V9WqgHzb8t1ZoUi+EX13ZhaXpB5m1apfX4RhjAlRJicR3taMhOMvioqpHABsXWktc\nn9KG7tFN+OOcNI6dyvM6HGNMACopkewUkftF5FqcvpFPAESkHrYeSa0RHOR0vO85fIpX5tsCWMaY\n85WUSO4EugK3Azeq6iH39f7AW2U5uYgMF5GNIrJFRB4tYr+IyEvu/tW+i2mJyFQRyRaRtUUcd7+I\nbBCRdSLy57LEYi5c77ZNGZMczZSvvyd93zGvwzHGBJiSRm1lq+q9qjpaVT/zeX2+qj5T2olFJBh4\nBRiBM1x4nIgUHjY8AujkPiYAr/rsexsYXsR5BwGjgZ6q2hUoNRZTcY8Ojye0ThC/m73e61CMMQGm\npKV2Z5V0oKpeU8q5+wJbVHWbe74ZOAnA95toNDBNnSFBi0UkXERaqWqWqi4UkbgizjsRmKSqp9w4\nskuJw1SCqMZhPDCkI3+cs4H5G7IZFB/ldUjGmABR0p3tFwE7genAd5R/fq1o9/gzMnBGfJVWJhrI\nKuG8nYHLROQPwEngl6q6tHAhEZmAU8shNja2nKGbotx+cTtmLNnJb2ev55KOkYTWsVUIjDEl95G0\nBB4HugEvAlcA+1T1K1X9qiqCK0YdIAKnr+YR4H0pYqFxVX1DVVNUNaV58+ZVHWONFFoniN9cncj3\n+44x9b/fex2OMSZAlNRHkq+qn6jqbThf2luABeVYiyQTaOOzHeO+Vt4yhWUAH6pjCc5Q5MgyxmQq\naGCXKIYmtODlLzez5/BJr8MxxgSAEtsmRKSuiIwB/g7cB7wEfFTGcy8FOolIOxEJBW4CCve7zAJu\ndUdv9QdyVLWkZi2AfwGD3Pg6A6HAvjLGZCrBE6MSyM1X/tcWwDLGUPIUKdOAb3HuIXlaVfuo6u9U\ntbQaAwCqmgf8DPgUSAPeV9V1InKviNzrFpsDbMOp7UwGfurz/tPd9+8iIhkicqe7ayrQ3h0WPAO4\nTW3+jirVtlkD7h7Qjg9XZJK6/YDX4RhjPCbFfQeLSAFw5qYB30ICqKo29nNslSYlJUWXLVvmdRg1\nyrFTeQx59isiG4Xy8X2XEhxka50ZU9OISKqqppRWrqQ+kiBVbeQ+Gvs8GlWnJGL8o0HdOjx+VQJr\nMw/z/rKdpR9gjKmxbPymuWBX92hF37gI/vLpRnKO55Z+gDGmRrJEYi7YmQWwDh0/zfNfbPI6HGOM\nRyyRmApJbN2YW/q15Z3F29mw+7DX4RhjPGCJxFTYw1d0plFYHZ6etd4WwDKmFrJEYiqsaYNQfjGs\nC99u28+cNbu9DscYU8UskZhKcXPfWBJaNeaJj9fy8pebyTx0wuuQjDFVxBKJqRTBQcILNybRuUVD\nnv18E5f+7zxumbKYD5dncPy0raxoTE1W7A2JNYndkFi1dh44zofLM5m5fCc7D5ygQWgwV/Voxdje\nbegT15Qi5tg0xgSgst6QaInE+E1BgbI0/QAzUzOYsyaLY6fziY2oz3XJMYxJjqZNRH2vQzTGlMAS\niQ9LJN47fjqPT9buZmZqBt9s3Q9A//YRjO3dhhHdWtKgbklL4xhjvGCJxIclksCScfA4Hy3PZOby\nDLbvP0790GBGdm/Fdckx9GsXQZDN22VMQLBE4sMSSWBSVVK3H2RmagazV2dx9FQeMU3rcV1yDNcl\nxxDbzJq+jPGSJRIflkgC34nT+Xy23mn6WrRlH6rQt10EY3vHMLJ7Kxpa05cxVc4SiQ9LJNXLrkMn\n+GhFJh+kZrBt3zHqhQQzoltLxvaOoX/7Ztb0ZUwVsUTiwxJJ9aSqLN9xyGn6WrWLI6fyiA6vx5jk\naK5LjiEusoHXIRpTo1ki8WGJpPo7mZvPZ+v3OE1fm/dSoNAnrinXJcdwVY9WNAoL8TpEY2ocSyQ+\nLJHULLtzTvLRikxmpu5k695jhIUEMbxrS67rHcPFHSJttUZjKoklEh+WSGomVWXlzkN8sDyDWSt3\ncfhkHq2ahJ1t+mrfvKHXIRpTrQVEIhGR4cCLQDAwRVUnFdov7v6RwHHgdlVd7u6bCowCslW1m88x\nTwF3A3vdlx5X1TklxWGJpOY7mZvPF2l7+CA1g682OU1fybHhjO3dhqt6tKJJPWv6Mqa8PE8kIhIM\nbAKuADKApcA4VV3vU2YkcD9OIukHvKiq/dx9A4CjwLQiEslRVX2mrLFYIqldsg+fafrKYHP2UerW\nCWJYV2fU16UdrenLmLIqayLx5+D8vsAWVd3mBjQDGA2s9ykzGidRKLBYRMJFpJWqZqnqQhGJ82N8\npoaKahzGPZd3YMKA9qzJzGFmagYfr9zFv1ftokXjulzbK4axvaPpGNXI61CNqRH8mUiigZ0+2xk4\ntY7SykQDWaWc+34RuRVYBvxCVQ9WMFZTA4kIPWLC6RETzv9clcC8tGxmpmYw+ettvPbVVpLahHNd\n7xiu6dGaJvWt6cuYC1Ud1yN5FWgPJOEknGeLKiQiE0RkmYgs27t3b1FFTC1St04wI7q34s3b+/Dt\nY4P59VUJnDidzxP/WkufP37Bfe8tZ/6GbPLyC7wO1Zhqx581kkygjc92jPtaecucQ1X3nHkuIpOB\n2cWUewN4A5w+kjJHbWq8qEZh3HVZe+68tB3rdh12m74y+c/qLJo3qsuYXtFc1zuGzi2s6cuYsvBn\nIlkKdBKRdjjJ4Sbg5kJlZgE/c/tP+gE5qlpis9aZPhR381pgbeWGbWoLEaFbdBO6RTfh8ZEJzNvg\nNH29ueh7Xl+4jZ4xTZymr56tCa8f6nW4xgQsfw//HQm8gDP8d6qq/kFE7gVQ1dfc4b9/BYbjDP8d\nr6rL3GOnAwOBSGAP8KSqviki7+A0aymQDtxTWvKxUVumPPYdPcXHK3cxMzWDtKzDhAYHMTQxiuuS\nY7i8c3PqBFfHFmFjys/z4b+BxBKJuVDrduXwQWom/1qZyYFjp4lsWJdre7Xmut4xxLds7HV4xviV\nJRIflkhMRZ3OK2DBxmw+WJ7Bl2nZ5BUo3aIbMzY5hmuSooloYE1fpuaxROLDEompTPuPnmLWql18\nsDyDtZmHCQkWBsdHMbZ3GwZ2aU6INX2ZGsISiQ9LJMZf0rIO80FqBv9amcm+o6dp1iCU0UnRjO0d\nQ2Jra/oy1ZslEh+WSIy/5eYXsHDTXmamZvBF2h5y85XEVo25rncMo5NaE9mwrtchGlNulkh8WCIx\nVengsdP8e7Uz6mt1Rg51goRB8c6or8HxUYTWsaYvUz1YIvFhicR4ZdOeI3yQmsGHKzLZe+QUTeuH\nnG366tq6Mc4IeGMCkyUSH5ZIjNfy8gv4evM+Zi7P4PN1ezidX0B8y0aMSY5mWGJLWzbYBCRLJD4s\nkZhAcuj4af69OouZqRms2nkIgPbNGzA0oQWD46NIadvUbno0AcESiQ9LJCZQ7dh/nHkb9vDlhmwW\nb9tPbr7SOKwOA7tEMSQhioGdo2xmYuMZSyQ+LJGY6uDoqTwWbd7LF2nZzN+Qzf5jpwkOElLaNmVI\nQhRDElrQPrKB9auYKmOJxIclElPdFBQoKzMOMS8tmy/S9rBh9xEA4prVZ0hCC4bER9GnXYTd/Gj8\nyhKJD0skprrLPHSCeWlOE9g3W/dzOq+ARnXrMKBLc4bERzGoSxRNbZoWU8kskfiwRGJqkmOn8vjv\nln18mZbNvI3Z7D1yiiCB5NimTm0lIYpOUQ2tCcxUmCUSH5ZITE1VUKCsyczhyw3ZfJm2h3W7DgPQ\nJqIeQ+KdpNK3XQR16wR7HKmpjiyR+LBEYmqL3TknmecmlUVb9nEqr4AGocEM6NycwfFRDIqPsula\nTJlZIvFhicTURidO5/PN1n18uSGbeWnZ7D58EhFIahPOkHhnFFh8y0bWBGaKZYnEhyUSU9upKut2\nHXb6VTbsYVVGDgDR4fUYHB/F4IQoLmrfjLAQawIzP7BE4sMSiTHnyj5ykvkbsvkyLZuvN+/jRG4+\n9UKCubRTJEMTnFFgUY3DvA7TeMwSiQ9LJMYU72RuPou37efLNKdvZVfOSQB6xjRhsNthbxNM1k6W\nSHxYIjGmbFSVDbuPMG+DcyPkyp2HUIWWjcMYFB/F0IQoLu4QSb1QawKrDQIikYjIcOBFIBiYoqqT\nCu0Xd/9I4Dhwu6oud/dNBUYB2ararYhz/wJ4BmiuqvtKisMSiTEXZt/RU8zfkM28Ddks3LSXY6fz\nqVsniEs7RjI4IYoh8S1o2cSawGoqzxOJiAQDm4ArgAxgKTBOVdf7lBkJ3I+TSPoBL6pqP3ffAOAo\nMK1wIhGRNsAUIB7obYnEGP87lZfPku8P8KU7bUvGwRMAdG3d+Oy0Ld2jmxAUZE1gNUUgJJKLgKdU\n9Up3+zEAVf2TT5nXgQWqOt3d3ggMVNUsdzsOmF1EIpkJ/A74GEixRGJM1VJVtmQf5Qt3FFjq9oMU\nKDRvVJfBXZxRYJd1iqR+aB2vQzUVUNZE4s9/5Whgp892Bk6to7Qy0UBWcScVkdFApqquKqnzT0Qm\nABMAYmNjyxW4MaZkIkKnFo3o1KIREwd24MCx03y1KZsv0rKZsyaLfyzbSWidIC5q34yhCVEMTmhB\ndHg9r8M2flKtfi6ISH3gcWBYaWVV9Q3gDXBqJH4OzZhaLaJBKNf2iuHaXjHk5hewNP3A2VFgT3y8\njic+Xkd8y0Znp8PvGRNOsDWB1Rj+TCSZQBuf7Rj3tfKW8dUBaAecqY3EAMtFpK+q7q5wxMaYCgsJ\nDuLiDpFc3CGSJ0YlsnXv0bPT4b/21TZemb+VZg1CGRQfxZD4KC7r3JyGdavVb1pTiD//9ZYCnUSk\nHU5yuAm4uVCZWcDPRGQGTrNXzpn+kaKo6hog6sy2iKRThj4SY4x3OjRvSIfmDbl7QHtyjueyYJMz\nCuzz9XuYmZpBSLDQv32zs9O2tImo73XIppz8Pfx3JPACzvDfqar6BxG5F0BVX3OH//4VGI4z/He8\nqi5zj50ODAQigT3Ak6r6ZqHzp2Od7cZUS3n5BaRuP3h25uKte48B0Cmq4dnp8JNjm1oTmIc8H7UV\nSCyRGBP40vcdO5tUlnx/gLwCJbx+CIPc9esHdG5O4zBbv74qWSLxYYnEmOrl8Mlcvt60jy/T9jB/\nYzYHj+dSJ0hIiWtK37gIesU2pVdsOOH1bVVIf7JE4sMSiTHVV36BsmKH0wT21ca9bNh9mAL3a6t9\n8wYkxzZ1Hm3D6RTVyJrCKpElEh+WSIypOY6dymNVxiFW7DjEih0HWb7jEAeOnQagYd069GzT5Gxy\nsVpLxQTCDYnGGFPpGtStc3Z4MTh32W/ff5zlOw6yfMdBVuw4xP8t2Eq+W21p37wBvdo4NZbk2KZ0\nbmG1lspmNRJjTI1z7FQeqzNy3MRSfK2lV2w4vdo0pWkDq7UUxWokxphaq0HdOlzUoRkXdWgGOLWW\nHQfcWsv2QyzfcfDcWktkA3rFWq3lQlmNxBhTKx0/nceqnTms2OkklxU7DrLfrbU0CA2mZ5vws534\ntbXWYjUSY4wpQf3Q4mstK3Y4tZZXv/qh1tIusgG9YsPPduR3aWm1ljOsRmKMMcU4fvqHvpbiai1n\nkkuv2KZE1LBai9VIjDGmguqH1qF/+2b0b/9DrWXngRNnR4gt33GQ177aVmStpVdsOF1aNKJOcJCX\nH6FKWCIxxpgyEhFim9Untll9ftQrGnBqLWsycljuNoct3LSXD5c7k5jXDw2mZ0z42U78mlhrAUsk\nxhhTIfVD69CvfTP6FVFrOTP02LfWEtesvpNU2jYluYbUWiyRGGNMJSqq1nLidD6rMw79UGvZvJcP\nV/xQa+kRc+7d+M0a1vXyI5SbJRJjjPGzeqHB59VaMg66fS3bnVrLGwu3kVe41hIbTq/YpsS3DOxa\niyUSY4ypYiJCm4j6tImoz+ikH2otazJzziaXhZv3FVlr6RXrNIkFUq3FEokxxgSAeqHB9G0XQd92\nEcC5tZYz97X41lraurWW5ACotVgiMcaYAFRarWXFjoMs2rKPj9xaS70Qt9bS9oe+lsgqqrVYIjHG\nmGqiLLWWyT61ltiI+ky6rvvZmZL9xRKJMcZUU0XVWk7murWW7c4Nk1GNwvweh18TiYgMB14EgoEp\nqjqp0H5x948EjgO3q+pyd99UYBSQrardfI75HTAaKACy3WN2+fNzGGNMdREWEkyfuAj6xEVU2Xv6\nrWdGRIKBV4ARQCIwTkQSCxUbAXRyHxOAV332vQ0ML+LUf1HVHqqaBMwGflPJoRtjjCkHf3bx9wW2\nqOo2VT0NzMCpSfgaDUxTx2IgXERaAajqQuBA4ZOq6mGfzQZAzZ910hhjApg/m7aigZ0+2xlAvzKU\niQaySjqxiPwBuBXIAQYVU2YCTi2H2NjY8sRtjDGmHAL3VskSqOr/qGob4F3gZ8WUeUNVU1Q1pXnz\n5lUboDHG1CL+TCSZQBuf7Rj3tfKWKcm7wHUXFJ0xxphK4c9EshToJCLtRCQUuAmYVajMLOBWcfQH\nclS1tGatTj6bo4ENlRm0McaY8vFbH4mq5onIz4BPcYb/TlXVdSJyr7v/NWAOztDfLTjDf8efOV5E\npgMDgUgRyQCeVNU3gUki0gVn+O924F5/fQZjjDGls6V2jTHGFKmsS+3WikQiIntxai8XIhLYV4nh\nVBaLq3wsrvKxuMonUOOCisXWVlVLHa1UKxJJRYjIsrJk5KpmcZWPxVU+Flf5BGpcUDWxVcvhv8YY\nYwKHJRJjjDEVYomkdG94HUAxLK7ysbjKx+Iqn0CNC6ogNusjMcYYUyFWIzHGGFMhlkiMMcZUiCUS\nl4gMF5GNIrJFRB4tYr+IyEvu/tUikhwgcQ0UkRwRWek+/L4+i4hMFZFsEVlbzH6vrlVpcVX5tXLf\nt42IzBeR9SKyTkR+XkSZKr9mZYzLi7+vMBFZIiKr3LieLqKMF9erLHF58jfmvnewiKwQkdlF7PPv\n9VLVWv/AmcJlK9AeCAVWAYmFyowE5gIC9Ae+C5C4BgKzq/h6DQCSgbXF7K/ya1XGuKr8Wrnv2wpI\ndp83AjYFyN9XWeLy4u9LgIbu8xDgO6B/AFyvssTlyd+Y+94PA+8V9f7+vl5WI3FUaBEuj+OqclrM\nomM+vLhWZYnLE6qape4S0qp6BEjDWXfHV5VfszLGVeXca3DU3QxxH4VHBXlxvcoSlydEJAa4CphS\nTBG/Xi9LJI7iFtgqbxkv4gK42K2uzhWRrn6OqSy8uFZl5em1EpE4oBfOr1lfnl6zEuICD66Z20yz\nEsgGPlfVgLheZYgLvPkbewH4Fc5ktkXx6/WyRFL9LQdiVbUH8DLwL4/jCWSeXisRaQh8ADyo5y4Z\n7alS4vLkmqlqvqom4axR1FdEulXF+5amDHFV+fUSkVFAtqqm+vu9imOJxFEVi3D5JS5VPXymuq2q\nc4AQEYn0c1yl8eJalcrLayUiIThf1u+q6odFFPHkmpUWl9d/X6p6CJgPDC+0y9O/seLi8uh6XQJc\nIyLpOM3fg0Xk74XK+PV6WSJx+GURrqqIS0Raioi4z/vi/Jvu93NcpfHiWpXKq2vlvuebQJqqPldM\nsSq/ZmWJy4trJiLNRSTcfV4PuILzF7Dz4nqVGpcX10tVH1PVGFWNw/mOmKeqPy5UzK/Xy28LW1Un\nWsFFuDyOaywwUUTygBPATeoO0/AXKWLRMZyOR8+uVRnjqvJr5boE+Amwxm1fB3gciPWJzYtrVpa4\nvLhmrYC/iUgwzhfx+6o62+v/H8sYl1d/Y+epyutlU6QYY4ypEGvaMsYYUyGWSIwxxlSIJRJjjDEV\nYonEGGNMhVgiMcYYUyGWSIypBCKSLz/M+LpSipipuQLnjpNiZjQ2JhDYfSTGVI4T7tQZxtQ6ViMx\nxtSK13MAAAF/SURBVI9EJF1E/iwia8RZy6Kj+3qciMxzJ/f7UkRi3ddbiMhH4qx5sUpELnZPFSwi\nk8VZB+Mz985qYwKCJRJjKke9Qk1bN/rsy1HV7sBfcWZpBWdCv7+5k/u9C7zkvv4S8JWq9sRZW2Wd\n+3on4BVV7QocAq7z8+cxpszsznZjKoGIHFXVhkW8ng4MVtVt7gSJu1W1mYjsA1qpaq77epaqRorI\nXiBGVU/5nCMOZ8ryTu72/wNCVPX3/v9kxpTOaiTG+J8W87w8Tvk8z8f6N00AsURijP/d6PPfb93n\n3+DM1ApwC/C1+/xLYCKcXUSpSVUFacyFsl81xlSOej4z6PL/27lDGwSCIAqgf0qgl2sGjUIQFM1g\naIM6aAN6WMTdBYeZXM68J1et+/k7m0nyHGOsX4APVfXK3CqOy9klyaOqbkne+W1jvSa5V9Upc/M4\nJ9l9BT/8Y0YCG1pmJNMY47P3XWArnrYAaNFIAGjRSABoESQAtAgSAFoECQAtggSAli9r86/tjidu\n0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70a806b208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model MSE Loss')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Set', 'Validation Set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "samples = []\n",
    "with open('./default_data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './default_data' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "                \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.1868Epoch 00000: val_loss improved from inf to 0.05009, saving model to ngen-improvement-00-0.05.hdf5\n",
      "105096/105096 [==============================] - 137s - loss: 0.0569 - acc: 0.1868 - val_loss: 0.0501 - val_acc: 0.1881\n",
      "Epoch 2/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.1871Epoch 00001: val_loss did not improve\n",
      "105096/105096 [==============================] - 136s - loss: 0.0547 - acc: 0.1870 - val_loss: 0.0503 - val_acc: 0.1880\n",
      "Epoch 3/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.1873Epoch 00002: val_loss improved from 0.05009 to 0.04895, saving model to ngen-improvement-02-0.05.hdf5\n",
      "105096/105096 [==============================] - 135s - loss: 0.0534 - acc: 0.1872 - val_loss: 0.0489 - val_acc: 0.1885\n",
      "Epoch 4/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.1877Epoch 00003: val_loss did not improve\n",
      "105096/105096 [==============================] - 135s - loss: 0.0525 - acc: 0.1876 - val_loss: 0.0495 - val_acc: 0.1885\n",
      "Epoch 5/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.1873Epoch 00004: val_loss improved from 0.04895 to 0.04795, saving model to ngen-improvement-04-0.05.hdf5\n",
      "105096/105096 [==============================] - 135s - loss: 0.0518 - acc: 0.1873 - val_loss: 0.0479 - val_acc: 0.1886\n",
      "Epoch 6/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.18778 ETA: 11s - loss: 0.0511 - a - ETA: 10s - loss: 0.0511 - Epoch 00005: val_loss did not improve\n",
      "105096/105096 [==============================] - 135s - loss: 0.0509 - acc: 0.1877 - val_loss: 0.0485 - val_acc: 0.1885\n",
      "Epoch 7/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.1877Epoch 00006: val_loss improved from 0.04795 to 0.04742, saving model to ngen-improvement-06-0.05.hdf5\n",
      "105096/105096 [==============================] - 136s - loss: 0.0507 - acc: 0.1876 - val_loss: 0.0474 - val_acc: 0.1886\n",
      "Epoch 8/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.1881Epoch 00007: val_loss improved from 0.04742 to 0.04725, saving model to ngen-improvement-07-0.05.hdf5\n",
      "105096/105096 [==============================] - 136s - loss: 0.0495 - acc: 0.1880 - val_loss: 0.0473 - val_acc: 0.1883\n",
      "Epoch 9/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.1881Epoch 00008: val_loss did not improve\n",
      "105096/105096 [==============================] - 134s - loss: 0.0493 - acc: 0.1881 - val_loss: 0.0475 - val_acc: 0.1885\n",
      "Epoch 10/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.1879Epoch 00009: val_loss improved from 0.04725 to 0.04615, saving model to ngen-improvement-09-0.05.hdf5\n",
      "105096/105096 [==============================] - 136s - loss: 0.0485 - acc: 0.1879 - val_loss: 0.0461 - val_acc: 0.1887\n",
      "Epoch 11/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.1879Epoch 00010: val_loss improved from 0.04615 to 0.04585, saving model to ngen-improvement-10-0.05.hdf5\n",
      "105096/105096 [==============================] - 136s - loss: 0.0479 - acc: 0.1879 - val_loss: 0.0458 - val_acc: 0.1889\n",
      "Epoch 12/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.1884Epoch 00011: val_loss did not improve\n",
      "105096/105096 [==============================] - 134s - loss: 0.0474 - acc: 0.1884 - val_loss: 0.0462 - val_acc: 0.1887\n",
      "Epoch 13/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.1886- ETA: 9s - loss: 0.04 - ETA: 6s - loss: 0.0466 - ETA: 3s - loss: 0.0465 - acc: 0.18 - ETA: 2s - loss: 0.0465 - ETA: 0s - loss: 0.0465 - acc: 0.1886Epoch 00012: val_loss did not improve\n",
      "105096/105096 [==============================] - 134s - loss: 0.0465 - acc: 0.1885 - val_loss: 0.0461 - val_acc: 0.1891\n",
      "Epoch 14/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.1884Epoch 00013: val_loss did not improve\n",
      "105096/105096 [==============================] - 135s - loss: 0.0464 - acc: 0.1883 - val_loss: 0.0462 - val_acc: 0.1886\n",
      "Epoch 15/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.1885Epoch 00014: val_loss did not improve\n",
      "105096/105096 [==============================] - 136s - loss: 0.0460 - acc: 0.1885 - val_loss: 0.0465 - val_acc: 0.1884\n",
      "Epoch 16/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.1885Epoch 00015: val_loss improved from 0.04585 to 0.04556, saving model to ngen-improvement-15-0.05.hdf5\n",
      "105096/105096 [==============================] - 133s - loss: 0.0457 - acc: 0.1885 - val_loss: 0.0456 - val_acc: 0.1890\n",
      "Epoch 17/17\n",
      "105024/105096 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.1889Epoch 00016: val_loss improved from 0.04556 to 0.04548, saving model to ngen-improvement-16-0.05.hdf5\n",
      "105096/105096 [==============================] - 136s - loss: 0.0450 - acc: 0.1889 - val_loss: 0.0455 - val_acc: 0.1890\n"
     ]
    }
   ],
   "source": [
    "# compile and train the model using the generator function\n",
    "\n",
    "train_generator = generator(train_samples, batch_size=32, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=32, colour_space='RGB')\n",
    "\n",
    "ngen = Sequential()\n",
    "\n",
    "ngen.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.2\n",
    "\n",
    "ngen.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "\n",
    "ngen.add(Flatten())\n",
    "ngen.add(Dense(100))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Dense(50))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Dense(10))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Dense(1))\n",
    "\n",
    "ngen.load_weights(\"ngen-improvement-00-0.05.hdf5\")\n",
    "ngen.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"ngen-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "ngen.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=17,\n",
    "                     callbacks=callbacks_list)\n",
    "\n",
    "ngen.save('ngen_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.1798Epoch 00000: val_loss improved from inf to 0.01627, saving model to ngen-improvement-all00-0.02.hdf5\n",
      "38568/38568 [==============================] - 51s - loss: 0.0172 - acc: 0.1798 - val_loss: 0.0163 - val_acc: 0.1851\n",
      "Epoch 2/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.1798Epoch 00001: val_loss improved from 0.01627 to 0.01613, saving model to ngen-improvement-all01-0.02.hdf5\n",
      "38568/38568 [==============================] - 49s - loss: 0.0167 - acc: 0.1798 - val_loss: 0.0161 - val_acc: 0.1851\n",
      "Epoch 3/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.1798Epoch 00002: val_loss improved from 0.01613 to 0.01609, saving model to ngen-improvement-all02-0.02.hdf5\n",
      "38568/38568 [==============================] - 48s - loss: 0.0166 - acc: 0.1798 - val_loss: 0.0161 - val_acc: 0.1851\n",
      "Epoch 4/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.1798Epoch 00003: val_loss improved from 0.01609 to 0.01598, saving model to ngen-improvement-all03-0.02.hdf5\n",
      "38568/38568 [==============================] - 49s - loss: 0.0164 - acc: 0.1798 - val_loss: 0.0160 - val_acc: 0.1851\n",
      "Epoch 5/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.1798Epoch 00004: val_loss did not improve\n",
      "38568/38568 [==============================] - 48s - loss: 0.0163 - acc: 0.1798 - val_loss: 0.0160 - val_acc: 0.1851\n",
      "Epoch 6/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.1798- ETA: 4s - loss: 0.0165 - ETA: 1s - loss: 0.0163 - acEpoch 00005: val_loss improved from 0.01598 to 0.01592, saving model to ngen-improvement-all05-0.02.hdf5\n",
      "38568/38568 [==============================] - 48s - loss: 0.0162 - acc: 0.1798 - val_loss: 0.0159 - val_acc: 0.1851\n",
      "Epoch 7/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.1797Epoch 00006: val_loss improved from 0.01592 to 0.01577, saving model to ngen-improvement-all06-0.02.hdf5\n",
      "38568/38568 [==============================] - 48s - loss: 0.0161 - acc: 0.1798 - val_loss: 0.0158 - val_acc: 0.1851\n",
      "Epoch 8/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.1797Epoch 00007: val_loss did not improve\n",
      "38568/38568 [==============================] - 48s - loss: 0.0160 - acc: 0.1798 - val_loss: 0.0158 - val_acc: 0.1851\n",
      "Epoch 9/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.1797Epoch 00008: val_loss did not improve\n",
      "38568/38568 [==============================] - 48s - loss: 0.0159 - acc: 0.1798 - val_loss: 0.0159 - val_acc: 0.1851\n",
      "Epoch 10/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.1798Epoch 00009: val_loss improved from 0.01577 to 0.01575, saving model to ngen-improvement-all09-0.02.hdf5\n",
      "38568/38568 [==============================] - 48s - loss: 0.0158 - acc: 0.1798 - val_loss: 0.0158 - val_acc: 0.1851\n",
      "Epoch 11/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.1798Epoch 00010: val_loss improved from 0.01575 to 0.01553, saving model to ngen-improvement-all10-0.02.hdf5\n",
      "38568/38568 [==============================] - 48s - loss: 0.0157 - acc: 0.1798 - val_loss: 0.0155 - val_acc: 0.1851\n",
      "Epoch 12/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.1798Epoch 00011: val_loss did not improve\n",
      "38568/38568 [==============================] - 49s - loss: 0.0157 - acc: 0.1798 - val_loss: 0.0156 - val_acc: 0.1851\n",
      "Epoch 13/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.1798Epoch 00012: val_loss did not improve\n",
      "38568/38568 [==============================] - 49s - loss: 0.0156 - acc: 0.1798 - val_loss: 0.0156 - val_acc: 0.1851\n",
      "Epoch 14/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.1798Epoch 00013: val_loss did not improve\n",
      "38568/38568 [==============================] - 49s - loss: 0.0155 - acc: 0.1798 - val_loss: 0.0156 - val_acc: 0.1851\n",
      "Epoch 15/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.1798Epoch 00014: val_loss improved from 0.01553 to 0.01547, saving model to ngen-improvement-all14-0.02.hdf5\n",
      "38568/38568 [==============================] - 49s - loss: 0.0155 - acc: 0.1798 - val_loss: 0.0155 - val_acc: 0.1851\n",
      "Epoch 16/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.1798Epoch 00015: val_loss did not improve\n",
      "38568/38568 [==============================] - 48s - loss: 0.0154 - acc: 0.1798 - val_loss: 0.0155 - val_acc: 0.1851\n",
      "Epoch 17/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.1797Epoch 00016: val_loss did not improve\n",
      "38568/38568 [==============================] - 49s - loss: 0.0153 - acc: 0.1798 - val_loss: 0.0157 - val_acc: 0.1851\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=32, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=32, colour_space='RGB')\n",
    "\n",
    "ngen.load_weights(\"ngen-improvement-16-0.05.hdf5\")\n",
    "ngen.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"ngen-improvement-all{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "ngen.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=17,\n",
    "                     callbacks=callbacks_list)\n",
    "\n",
    "ngen.save('ngen_model_trained_on_all_sets.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL with New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "samples = []\n",
    "with open('./new_data/driving_log_1.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "with open('./new_data/driving_log_2.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './new_data' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "                \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "20736/73488 [=======>......................] - ETA: 122s - loss: 0.0340 - acc: 0.0062\n",
      "Epoch 3/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.0066Epoch 00002: val_loss improved from 0.01961 to 0.01874, saving model to ngen-new-data02-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0234 - acc: 0.0066 - val_loss: 0.0187 - val_acc: 0.0069\n",
      "Epoch 4/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.0066Epoch 00003: val_loss improved from 0.01874 to 0.01815, saving model to ngen-new-data03-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0224 - acc: 0.0066 - val_loss: 0.0182 - val_acc: 0.0069\n",
      "Epoch 5/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.0066Epoch 00004: val_loss improved from 0.01815 to 0.01775, saving model to ngen-new-data04-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0216 - acc: 0.0066 - val_loss: 0.0178 - val_acc: 0.0069\n",
      "Epoch 6/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.0066Epoch 00005: val_loss improved from 0.01775 to 0.01745, saving model to ngen-new-data05-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0210 - acc: 0.0066 - val_loss: 0.0175 - val_acc: 0.0069\n",
      "Epoch 7/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.0066Epoch 00006: val_loss improved from 0.01745 to 0.01646, saving model to ngen-new-data06-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0205 - acc: 0.0066 - val_loss: 0.0165 - val_acc: 0.0069\n",
      "Epoch 8/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.0066Epoch 00007: val_loss did not improve\n",
      "73488/73488 [==============================] - 91s - loss: 0.0201 - acc: 0.0066 - val_loss: 0.0166 - val_acc: 0.0069\n",
      "Epoch 9/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.0066Epoch 00008: val_loss improved from 0.01646 to 0.01608, saving model to ngen-new-data08-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0197 - acc: 0.0066 - val_loss: 0.0161 - val_acc: 0.0069\n",
      "Epoch 10/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.0066Epoch 00009: val_loss did not improve\n",
      "73488/73488 [==============================] - 91s - loss: 0.0194 - acc: 0.0066 - val_loss: 0.0161 - val_acc: 0.0069\n",
      "Epoch 11/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.0066Epoch 00010: val_loss improved from 0.01608 to 0.01585, saving model to ngen-new-data10-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0191 - acc: 0.0066 - val_loss: 0.0158 - val_acc: 0.0069\n",
      "Epoch 12/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.0066Epoch 00011: val_loss improved from 0.01585 to 0.01555, saving model to ngen-new-data11-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0189 - acc: 0.0066 - val_loss: 0.0156 - val_acc: 0.0069\n",
      "Epoch 13/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.0066Epoch 00012: val_loss improved from 0.01555 to 0.01537, saving model to ngen-new-data12-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0184 - acc: 0.0066 - val_loss: 0.0154 - val_acc: 0.0069\n",
      "Epoch 14/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.0066Epoch 00013: val_loss did not improve\n",
      "73488/73488 [==============================] - 91s - loss: 0.0185 - acc: 0.0066 - val_loss: 0.0158 - val_acc: 0.0069\n",
      "Epoch 15/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.0066Epoch 00014: val_loss improved from 0.01537 to 0.01501, saving model to ngen-new-data14-0.02.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0181 - acc: 0.0066 - val_loss: 0.0150 - val_acc: 0.0069\n",
      "Epoch 16/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.0066Epoch 00015: val_loss did not improve\n",
      "73488/73488 [==============================] - 91s - loss: 0.0179 - acc: 0.0066 - val_loss: 0.0158 - val_acc: 0.0069\n",
      "Epoch 17/17\n",
      "73344/73488 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.0066Epoch 00016: val_loss improved from 0.01501 to 0.01480, saving model to ngen-new-data16-0.01.hdf5\n",
      "73488/73488 [==============================] - 91s - loss: 0.0179 - acc: 0.0066 - val_loss: 0.0148 - val_acc: 0.0069\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=32, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=32, colour_space='RGB')\n",
    "\n",
    "ngen = Sequential()\n",
    "\n",
    "ngen.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.2\n",
    "\n",
    "ngen.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "\n",
    "ngen.add(Flatten())\n",
    "ngen.add(Dense(100))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Dense(50))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Dense(10))\n",
    "ngen.add(Dropout(keep_prob))\n",
    "ngen.add(Dense(1))\n",
    "\n",
    "\n",
    "ngen.load_weights(\"./models/ngen_model_trained_on_all_sets.h5\")\n",
    "ngen.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"ngen-new-data{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "ngen.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=17,\n",
    "                     callbacks=callbacks_list)\n",
    "\n",
    "ngen.save('ngen_new_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using default data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "default_samples = []\n",
    "with open('./data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        default_samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(default_samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './data' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "                \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.1808Epoch 00000: val_loss improved from inf to 0.02151, saving model to default-intermediate-00-0.02.hdf5\n",
      "38568/38568 [==============================] - 54s - loss: 0.0284 - acc: 0.1806 - val_loss: 0.0215 - val_acc: 0.1814\n",
      "Epoch 2/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.1810Epoch 00001: val_loss improved from 0.02151 to 0.01750, saving model to default-intermediate-01-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0208 - acc: 0.1808 - val_loss: 0.0175 - val_acc: 0.1814\n",
      "Epoch 3/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.1810Epoch 00002: val_loss improved from 0.01750 to 0.01686, saving model to default-intermediate-02-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0187 - acc: 0.1808 - val_loss: 0.0169 - val_acc: 0.1814\n",
      "Epoch 4/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.1810Epoch 00003: val_loss improved from 0.01686 to 0.01667, saving model to default-intermediate-03-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0181 - acc: 0.1808 - val_loss: 0.0167 - val_acc: 0.1814\n",
      "Epoch 5/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.1810Epoch 00004: val_loss did not improve\n",
      "38568/38568 [==============================] - 46s - loss: 0.0177 - acc: 0.1808 - val_loss: 0.0169 - val_acc: 0.1814\n",
      "Epoch 6/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.1810Epoch 00005: val_loss improved from 0.01667 to 0.01627, saving model to default-intermediate-05-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0177 - acc: 0.1808 - val_loss: 0.0163 - val_acc: 0.1814\n",
      "Epoch 7/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.1810Epoch 00006: val_loss did not improve\n",
      "38568/38568 [==============================] - 47s - loss: 0.0174 - acc: 0.1808 - val_loss: 0.0166 - val_acc: 0.1814\n",
      "Epoch 8/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.1810Epoch 00007: val_loss improved from 0.01627 to 0.01614, saving model to default-intermediate-07-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0173 - acc: 0.1808 - val_loss: 0.0161 - val_acc: 0.1814\n",
      "Epoch 9/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.1810Epoch 00008: val_loss improved from 0.01614 to 0.01599, saving model to default-intermediate-08-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0172 - acc: 0.1808 - val_loss: 0.0160 - val_acc: 0.1814\n",
      "Epoch 10/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.1810Epoch 00009: val_loss improved from 0.01599 to 0.01598, saving model to default-intermediate-09-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0172 - acc: 0.1808 - val_loss: 0.0160 - val_acc: 0.1814\n",
      "Epoch 11/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.1810Epoch 00010: val_loss improved from 0.01598 to 0.01594, saving model to default-intermediate-10-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0170 - acc: 0.1807 - val_loss: 0.0159 - val_acc: 0.1814\n",
      "Epoch 12/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.1810Epoch 00011: val_loss improved from 0.01594 to 0.01590, saving model to default-intermediate-11-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0170 - acc: 0.1808 - val_loss: 0.0159 - val_acc: 0.1814\n",
      "Epoch 13/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.1810Epoch 00012: val_loss improved from 0.01590 to 0.01583, saving model to default-intermediate-12-0.02.hdf5\n",
      "38568/38568 [==============================] - 47s - loss: 0.0168 - acc: 0.1808 - val_loss: 0.0158 - val_acc: 0.1814\n",
      "Epoch 14/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.1810Epoch 00013: val_loss did not improve\n",
      "38568/38568 [==============================] - 46s - loss: 0.0168 - acc: 0.1807 - val_loss: 0.0159 - val_acc: 0.1814\n",
      "Epoch 15/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.1810Epoch 00014: val_loss improved from 0.01583 to 0.01553, saving model to default-intermediate-14-0.02.hdf5\n",
      "38568/38568 [==============================] - 46s - loss: 0.0167 - acc: 0.1807 - val_loss: 0.0155 - val_acc: 0.1814\n",
      "Epoch 16/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.1810Epoch 00015: val_loss did not improve\n",
      "38568/38568 [==============================] - 46s - loss: 0.0165 - acc: 0.1808 - val_loss: 0.0156 - val_acc: 0.1814\n",
      "Epoch 17/17\n",
      "38400/38568 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.1810Epoch 00016: val_loss did not improve\n",
      "38568/38568 [==============================] - 46s - loss: 0.0166 - acc: 0.1808 - val_loss: 0.0157 - val_acc: 0.1814\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ngen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f620e24fdc62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m                      callbacks=callbacks_list)\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mngen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'default_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ngen' is not defined"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=256, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=256, colour_space='RGB')\n",
    "\n",
    "default = Sequential()\n",
    "\n",
    "default.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.2\n",
    "\n",
    "default.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "default.add(Dropout(keep_prob))\n",
    "default.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "default.add(Dropout(keep_prob))\n",
    "default.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "default.add(Dropout(keep_prob))\n",
    "default.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "default.add(Dropout(keep_prob))\n",
    "default.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "default.add(Dropout(keep_prob))\n",
    "\n",
    "default.add(Flatten())\n",
    "default.add(Dense(100))\n",
    "default.add(Dropout(keep_prob))\n",
    "default.add(Dense(50))\n",
    "default.add(Dropout(keep_prob))\n",
    "default.add(Dense(10))\n",
    "default.add(Dropout(keep_prob))\n",
    "default.add(Dense(1))\n",
    "\n",
    "default.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"default-intermediate-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "default.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=17,\n",
    "                     callbacks=callbacks_list)\n",
    "\n",
    "default.save('default_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use New Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "samples = []\n",
    "with open('./new_data/driving_log_1.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "with open('./new_data/driving_log_2.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './new_data' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "                \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "73200/73488 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.0069Epoch 00000: val_loss improved from inf to 0.01838, saving model to new-data-model-2-try00-0.02.hdf5\n",
      "73488/73488 [==============================] - 96s - loss: 0.0231 - acc: 0.0069 - val_loss: 0.0184 - val_acc: 0.0057\n",
      "Epoch 2/8\n",
      "73200/73488 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.0069Epoch 00001: val_loss improved from 0.01838 to 0.01738, saving model to new-data-model-2-try01-0.02.hdf5\n",
      "73488/73488 [==============================] - 85s - loss: 0.0215 - acc: 0.0069 - val_loss: 0.0174 - val_acc: 0.0057\n",
      "Epoch 3/8\n",
      "73200/73488 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.0069Epoch 00002: val_loss did not improve\n",
      "73488/73488 [==============================] - 85s - loss: 0.0211 - acc: 0.0069 - val_loss: 0.0174 - val_acc: 0.0057\n",
      "Epoch 4/8\n",
      "73200/73488 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.0069Epoch 00003: val_loss improved from 0.01738 to 0.01732, saving model to new-data-model-2-try03-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0206 - acc: 0.0069 - val_loss: 0.0173 - val_acc: 0.0057\n",
      "Epoch 5/8\n",
      "73200/73488 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.0069Epoch 00004: val_loss improved from 0.01732 to 0.01692, saving model to new-data-model-2-try04-0.02.hdf5\n",
      "73488/73488 [==============================] - 85s - loss: 0.0200 - acc: 0.0069 - val_loss: 0.0169 - val_acc: 0.0057\n",
      "Epoch 6/8\n",
      "73200/73488 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.0069Epoch 00005: val_loss improved from 0.01692 to 0.01669, saving model to new-data-model-2-try05-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0197 - acc: 0.0069 - val_loss: 0.0167 - val_acc: 0.0057\n",
      "Epoch 7/8\n",
      "73200/73488 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.0069Epoch 00006: val_loss improved from 0.01669 to 0.01613, saving model to new-data-model-2-try06-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0190 - acc: 0.0069 - val_loss: 0.0161 - val_acc: 0.0057\n",
      "Epoch 8/8\n",
      "73200/73488 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.0069Epoch 00007: val_loss improved from 0.01613 to 0.01561, saving model to new-data-model-2-try07-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0187 - acc: 0.0069 - val_loss: 0.0156 - val_acc: 0.0057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f942c408a58>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=200, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=200, colour_space='RGB')\n",
    "\n",
    "new_data = Sequential()\n",
    "\n",
    "new_data.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.2\n",
    "\n",
    "new_data.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "new_data.add(Dropout(keep_prob))\n",
    "new_data.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "new_data.add(Dropout(keep_prob))\n",
    "new_data.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "new_data.add(Dropout(keep_prob))\n",
    "new_data.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "new_data.add(Dropout(keep_prob))\n",
    "new_data.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "new_data.add(Dropout(keep_prob))\n",
    "\n",
    "new_data.add(Flatten())\n",
    "new_data.add(Dense(100))\n",
    "new_data.add(Dropout(keep_prob))\n",
    "new_data.add(Dense(50))\n",
    "new_data.add(Dropout(keep_prob))\n",
    "new_data.add(Dense(10))\n",
    "new_data.add(Dropout(keep_prob))\n",
    "new_data.add(Dense(1))\n",
    "\n",
    "new_data.load_weights(\"./new-data-model06-0.02.hdf5\")\n",
    "new_data.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "filepath=\"new-data-model-2-try{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "new_data.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=8,\n",
    "                     callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default & New Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train default with new data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "samples = []\n",
    "with open('./new_data/driving_log_1.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "with open('./new_data/driving_log_2.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './new_data' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "                \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.0066Epoch 00000: val_loss improved from inf to 0.02938, saving model to default-new-data-model00-0.03.hdf5\n",
      "73488/73488 [==============================] - 94s - loss: 0.0393 - acc: 0.0066 - val_loss: 0.0294 - val_acc: 0.0057\n",
      "Epoch 2/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.0067Epoch 00001: val_loss improved from 0.02938 to 0.02440, saving model to default-new-data-model01-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0288 - acc: 0.0067 - val_loss: 0.0244 - val_acc: 0.0060\n",
      "Epoch 3/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.0068Epoch 00002: val_loss improved from 0.02440 to 0.02250, saving model to default-new-data-model02-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0257 - acc: 0.0068 - val_loss: 0.0225 - val_acc: 0.0060\n",
      "Epoch 4/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.0068Epoch 00003: val_loss improved from 0.02250 to 0.02073, saving model to default-new-data-model03-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0240 - acc: 0.0068 - val_loss: 0.0207 - val_acc: 0.0060\n",
      "Epoch 5/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.0068Epoch 00004: val_loss improved from 0.02073 to 0.02061, saving model to default-new-data-model04-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0227 - acc: 0.0068 - val_loss: 0.0206 - val_acc: 0.0060\n",
      "Epoch 6/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.0068Epoch 00005: val_loss improved from 0.02061 to 0.01933, saving model to default-new-data-model05-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0215 - acc: 0.0068 - val_loss: 0.0193 - val_acc: 0.0060\n",
      "Epoch 7/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.0068Epoch 00006: val_loss improved from 0.01933 to 0.01889, saving model to default-new-data-model06-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0208 - acc: 0.0068 - val_loss: 0.0189 - val_acc: 0.0060\n",
      "Epoch 8/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.0068Epoch 00007: val_loss improved from 0.01889 to 0.01829, saving model to default-new-data-model07-0.02.hdf5\n",
      "73488/73488 [==============================] - 87s - loss: 0.0204 - acc: 0.0068 - val_loss: 0.0183 - val_acc: 0.0060\n",
      "Epoch 9/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.0068Epoch 00008: val_loss improved from 0.01829 to 0.01735, saving model to default-new-data-model08-0.02.hdf5\n",
      "73488/73488 [==============================] - 87s - loss: 0.0197 - acc: 0.0068 - val_loss: 0.0173 - val_acc: 0.0060\n",
      "Epoch 10/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.0068Epoch 00009: val_loss improved from 0.01735 to 0.01672, saving model to default-new-data-model09-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0195 - acc: 0.0068 - val_loss: 0.0167 - val_acc: 0.0060\n",
      "Epoch 11/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.0068Epoch 00010: val_loss improved from 0.01672 to 0.01657, saving model to default-new-data-model10-0.02.hdf5\n",
      "73488/73488 [==============================] - 87s - loss: 0.0188 - acc: 0.0068 - val_loss: 0.0166 - val_acc: 0.0060\n",
      "Epoch 12/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.0068Epoch 00011: val_loss improved from 0.01657 to 0.01632, saving model to default-new-data-model11-0.02.hdf5\n",
      "73488/73488 [==============================] - 87s - loss: 0.0187 - acc: 0.0068 - val_loss: 0.0163 - val_acc: 0.0060\n",
      "Epoch 13/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.0068Epoch 00012: val_loss improved from 0.01632 to 0.01587, saving model to default-new-data-model12-0.02.hdf5\n",
      "73488/73488 [==============================] - 87s - loss: 0.0182 - acc: 0.0068 - val_loss: 0.0159 - val_acc: 0.0060\n",
      "Epoch 14/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.0068Epoch 00013: val_loss improved from 0.01587 to 0.01565, saving model to default-new-data-model13-0.02.hdf5\n",
      "73488/73488 [==============================] - 86s - loss: 0.0180 - acc: 0.0068 - val_loss: 0.0156 - val_acc: 0.0060\n",
      "Epoch 15/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.0068Epoch 00014: val_loss improved from 0.01565 to 0.01544, saving model to default-new-data-model14-0.02.hdf5\n",
      "73488/73488 [==============================] - 87s - loss: 0.0178 - acc: 0.0068 - val_loss: 0.0154 - val_acc: 0.0060\n",
      "Epoch 16/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.0068Epoch 00015: val_loss did not improve\n",
      "73488/73488 [==============================] - 87s - loss: 0.0176 - acc: 0.0068 - val_loss: 0.0157 - val_acc: 0.0060\n",
      "Epoch 17/17\n",
      "72960/73488 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.0068Epoch 00016: val_loss did not improve\n",
      "73488/73488 [==============================] - 87s - loss: 0.0176 - acc: 0.0068 - val_loss: 0.0155 - val_acc: 0.0060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93f01fe128>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=128, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=128, colour_space='RGB')\n",
    "\n",
    "default_new_data = Sequential()\n",
    "\n",
    "default_new_data.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.2\n",
    "\n",
    "default_new_data.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "default_new_data.add(Dropout(keep_prob))\n",
    "default_new_data.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "default_new_data.add(Dropout(keep_prob))\n",
    "default_new_data.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "default_new_data.add(Dropout(keep_prob))\n",
    "default_new_data.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "default_new_data.add(Dropout(keep_prob))\n",
    "default_new_data.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "default_new_data.add(Dropout(keep_prob))\n",
    "\n",
    "default_new_data.add(Flatten())\n",
    "default_new_data.add(Dense(100))\n",
    "default_new_data.add(Dropout(keep_prob))\n",
    "default_new_data.add(Dense(50))\n",
    "default_new_data.add(Dropout(keep_prob))\n",
    "default_new_data.add(Dense(10))\n",
    "default_new_data.add(Dropout(keep_prob))\n",
    "default_new_data.add(Dense(1))\n",
    "\n",
    "default_new_data.load_weights(\"./default-intermediate-14-0.02.hdf5\")\n",
    "default_new_data.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"default-new-data-model{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "default_new_data.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=17,\n",
    "                     callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training best model with 4th training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets used in transfer learning: 1st 1GB Keyboard Set, Default Set, 2nd 700MB Mouse Set, 3rd 700MB Mouse Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "samples = []\n",
    "with open('./data_4/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "with open('./data_4/driving_log_2.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './data_4' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "                \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.0057Epoch 00000: val_loss improved from inf to 0.01685, saving model to bm-00-0.02.hdf5\n",
      "84336/84336 [==============================] - 107s - loss: 0.0226 - acc: 0.0057 - val_loss: 0.0168 - val_acc: 0.0056\n",
      "Epoch 2/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.0057Epoch 00001: val_loss improved from 0.01685 to 0.01607, saving model to bm-01-0.02.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0205 - acc: 0.0057 - val_loss: 0.0161 - val_acc: 0.0056\n",
      "Epoch 3/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.0057Epoch 00002: val_loss improved from 0.01607 to 0.01565, saving model to bm-02-0.02.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0198 - acc: 0.0058 - val_loss: 0.0156 - val_acc: 0.0056\n",
      "Epoch 4/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.0058Epoch 00003: val_loss improved from 0.01565 to 0.01534, saving model to bm-03-0.02.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0193 - acc: 0.0058 - val_loss: 0.0153 - val_acc: 0.0056\n",
      "Epoch 5/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.0058Epoch 00004: val_loss improved from 0.01534 to 0.01495, saving model to bm-04-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0190 - acc: 0.0058 - val_loss: 0.0149 - val_acc: 0.0056\n",
      "Epoch 6/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.0058Epoch 00005: val_loss improved from 0.01495 to 0.01488, saving model to bm-05-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0188 - acc: 0.0058 - val_loss: 0.0149 - val_acc: 0.0056\n",
      "Epoch 7/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.0057Epoch 00006: val_loss improved from 0.01488 to 0.01483, saving model to bm-06-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0186 - acc: 0.0058 - val_loss: 0.0148 - val_acc: 0.0056\n",
      "Epoch 8/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.0058Epoch 00007: val_loss improved from 0.01483 to 0.01466, saving model to bm-07-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0183 - acc: 0.0058 - val_loss: 0.0147 - val_acc: 0.0056\n",
      "Epoch 9/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.0058Epoch 00008: val_loss improved from 0.01466 to 0.01439, saving model to bm-08-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0181 - acc: 0.0058 - val_loss: 0.0144 - val_acc: 0.0056\n",
      "Epoch 10/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.0058Epoch 00009: val_loss improved from 0.01439 to 0.01418, saving model to bm-09-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0178 - acc: 0.0058 - val_loss: 0.0142 - val_acc: 0.0056\n",
      "Epoch 11/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.0058Epoch 00010: val_loss did not improve\n",
      "84336/84336 [==============================] - 100s - loss: 0.0177 - acc: 0.0058 - val_loss: 0.0142 - val_acc: 0.0056\n",
      "Epoch 12/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.0058Epoch 00011: val_loss improved from 0.01418 to 0.01410, saving model to bm-11-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0176 - acc: 0.0058 - val_loss: 0.0141 - val_acc: 0.0056\n",
      "Epoch 13/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.0058Epoch 00012: val_loss did not improve\n",
      "84336/84336 [==============================] - 100s - loss: 0.0176 - acc: 0.0058 - val_loss: 0.0141 - val_acc: 0.0056\n",
      "Epoch 14/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.0058Epoch 00013: val_loss improved from 0.01410 to 0.01395, saving model to bm-13-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0173 - acc: 0.0058 - val_loss: 0.0139 - val_acc: 0.0056\n",
      "Epoch 15/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.0057Epoch 00014: val_loss did not improve\n",
      "84336/84336 [==============================] - 100s - loss: 0.0172 - acc: 0.0058 - val_loss: 0.0140 - val_acc: 0.0056\n",
      "Epoch 16/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.0058Epoch 00015: val_loss did not improve\n",
      "84336/84336 [==============================] - 100s - loss: 0.0170 - acc: 0.0058 - val_loss: 0.0140 - val_acc: 0.0056\n",
      "Epoch 17/17\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.0058Epoch 00016: val_loss improved from 0.01395 to 0.01368, saving model to bm-16-0.01.hdf5\n",
      "84336/84336 [==============================] - 100s - loss: 0.0171 - acc: 0.0058 - val_loss: 0.0137 - val_acc: 0.0056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa964491a58>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=128, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=128, colour_space='RGB')\n",
    "\n",
    "bm = Sequential()\n",
    "\n",
    "bm.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.2\n",
    "\n",
    "bm.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "\n",
    "bm.add(Flatten())\n",
    "bm.add(Dense(100))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Dense(50))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Dense(10))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Dense(1))\n",
    "\n",
    "bm.load_weights(\"./best_model.hdf5\")\n",
    "bm.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"bm-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "bm.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=17,\n",
    "                     callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training just for second track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "samples = []\n",
    "with open('./last/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './last' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "                \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0327 - acc: 0.0145Epoch 00000: val_loss improved from inf to 0.02096, saving model to t2-00-0.02.hdf5\n",
      "16104/16104 [==============================] - 23s - loss: 0.0323 - acc: 0.0145 - val_loss: 0.0210 - val_acc: 0.0099\n",
      "Epoch 2/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0292 - acc: 0.0145Epoch 00001: val_loss improved from 0.02096 to 0.01873, saving model to t2-01-0.02.hdf5\n",
      "16104/16104 [==============================] - 19s - loss: 0.0290 - acc: 0.0145 - val_loss: 0.0187 - val_acc: 0.0099\n",
      "Epoch 3/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0269 - acc: 0.0145Epoch 00002: val_loss improved from 0.01873 to 0.01807, saving model to t2-02-0.02.hdf5\n",
      "16104/16104 [==============================] - 19s - loss: 0.0268 - acc: 0.0145 - val_loss: 0.0181 - val_acc: 0.0099\n",
      "Epoch 4/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0260 - acc: 0.0145Epoch 00003: val_loss improved from 0.01807 to 0.01781, saving model to t2-03-0.02.hdf5\n",
      "16104/16104 [==============================] - 19s - loss: 0.0258 - acc: 0.0145 - val_loss: 0.0178 - val_acc: 0.0099\n",
      "Epoch 5/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0258 - acc: 0.0145Epoch 00004: val_loss improved from 0.01781 to 0.01649, saving model to t2-04-0.02.hdf5\n",
      "16104/16104 [==============================] - 20s - loss: 0.0257 - acc: 0.0145 - val_loss: 0.0165 - val_acc: 0.0099\n",
      "Epoch 6/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0249 - acc: 0.0145Epoch 00005: val_loss improved from 0.01649 to 0.01616, saving model to t2-05-0.02.hdf5\n",
      "16104/16104 [==============================] - 20s - loss: 0.0249 - acc: 0.0145 - val_loss: 0.0162 - val_acc: 0.0099\n",
      "Epoch 7/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0244 - acc: 0.0145Epoch 00006: val_loss did not improve\n",
      "16104/16104 [==============================] - 19s - loss: 0.0244 - acc: 0.0145 - val_loss: 0.0164 - val_acc: 0.0099\n",
      "Epoch 8/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0238 - acc: 0.0145Epoch 00007: val_loss improved from 0.01616 to 0.01536, saving model to t2-07-0.02.hdf5\n",
      "16104/16104 [==============================] - 19s - loss: 0.0237 - acc: 0.0145 - val_loss: 0.0154 - val_acc: 0.0099\n",
      "Epoch 9/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0236 - acc: 0.0145Epoch 00008: val_loss did not improve\n",
      "16104/16104 [==============================] - 19s - loss: 0.0235 - acc: 0.0145 - val_loss: 0.0158 - val_acc: 0.0099\n",
      "Epoch 10/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0233 - acc: 0.0145Epoch 00009: val_loss improved from 0.01536 to 0.01533, saving model to t2-09-0.02.hdf5\n",
      "16104/16104 [==============================] - 19s - loss: 0.0232 - acc: 0.0145 - val_loss: 0.0153 - val_acc: 0.0099\n",
      "Epoch 11/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0237 - acc: 0.0145Epoch 00010: val_loss did not improve\n",
      "16104/16104 [==============================] - 19s - loss: 0.0236 - acc: 0.0145 - val_loss: 0.0157 - val_acc: 0.0099\n",
      "Epoch 12/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0232 - acc: 0.0145Epoch 00011: val_loss improved from 0.01533 to 0.01442, saving model to t2-11-0.01.hdf5\n",
      "16104/16104 [==============================] - 19s - loss: 0.0232 - acc: 0.0145 - val_loss: 0.0144 - val_acc: 0.0099\n",
      "Epoch 13/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0230 - acc: 0.0145Epoch 00012: val_loss did not improve\n",
      "16104/16104 [==============================] - 19s - loss: 0.0229 - acc: 0.0145 - val_loss: 0.0147 - val_acc: 0.0099\n",
      "Epoch 14/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0226 - acc: 0.0145Epoch 00013: val_loss improved from 0.01442 to 0.01440, saving model to t2-13-0.01.hdf5\n",
      "16104/16104 [==============================] - 20s - loss: 0.0226 - acc: 0.0145 - val_loss: 0.0144 - val_acc: 0.0099\n",
      "Epoch 15/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0225 - acc: 0.0145Epoch 00014: val_loss improved from 0.01440 to 0.01419, saving model to t2-14-0.01.hdf5\n",
      "16104/16104 [==============================] - 19s - loss: 0.0224 - acc: 0.0145 - val_loss: 0.0142 - val_acc: 0.0099\n",
      "Epoch 16/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0224 - acc: 0.0145Epoch 00015: val_loss did not improve\n",
      "16104/16104 [==============================] - 19s - loss: 0.0224 - acc: 0.0145 - val_loss: 0.0149 - val_acc: 0.0099\n",
      "Epoch 17/17\n",
      "15360/16104 [===========================>..] - ETA: 0s - loss: 0.0221 - acc: 0.0145Epoch 00016: val_loss improved from 0.01419 to 0.01383, saving model to t2-16-0.01.hdf5\n",
      "16104/16104 [==============================] - 19s - loss: 0.0222 - acc: 0.0145 - val_loss: 0.0138 - val_acc: 0.0099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa99400b0b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=128, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=128, colour_space='RGB')\n",
    "\n",
    "t2 = Sequential()\n",
    "\n",
    "t2.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.2\n",
    "\n",
    "t2.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "\n",
    "t2.add(Flatten())\n",
    "t2.add(Dense(100))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Dense(50))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Dense(10))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Dense(1))\n",
    "\n",
    "t2.load_weights(\"./best_model.hdf5\")\n",
    "t2.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"t2-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "t2.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=17,\n",
    "                     callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve track 2 curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "samples = []\n",
    "with open('./ct/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './ct' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "               \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.2978 - acc: 0.1567 - val_loss: 0.0840 - val_acc: 0.2538\n",
      "Epoch 2/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.2383 - acc: 0.1816 - val_loss: 0.0517 - val_acc: 0.2609\n",
      "Epoch 3/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.2206 - acc: 0.1870 - val_loss: 0.0567 - val_acc: 0.2596\n",
      "Epoch 4/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.2119 - acc: 0.1896 - val_loss: 0.0755 - val_acc: 0.2590\n",
      "Epoch 5/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.2087 - acc: 0.1936 - val_loss: 0.0710 - val_acc: 0.2596\n",
      "Epoch 6/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.2050 - acc: 0.1915 - val_loss: 0.0456 - val_acc: 0.2628\n",
      "Epoch 7/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.2026 - acc: 0.1901 - val_loss: 0.0552 - val_acc: 0.2609\n",
      "Epoch 8/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1960 - acc: 0.1963 - val_loss: 0.0462 - val_acc: 0.2635\n",
      "Epoch 9/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1873 - acc: 0.2061 - val_loss: 0.0384 - val_acc: 0.2654\n",
      "Epoch 10/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1950 - acc: 0.1962 - val_loss: 0.0503 - val_acc: 0.2654\n",
      "Epoch 11/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1891 - acc: 0.2023 - val_loss: 0.0649 - val_acc: 0.2615\n",
      "Epoch 12/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1866 - acc: 0.1955 - val_loss: 0.0527 - val_acc: 0.2641\n",
      "Epoch 13/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1821 - acc: 0.2047 - val_loss: 0.0472 - val_acc: 0.2647\n",
      "Epoch 14/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1862 - acc: 0.2044 - val_loss: 0.0517 - val_acc: 0.2654\n",
      "Epoch 15/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1760 - acc: 0.2047 - val_loss: 0.0421 - val_acc: 0.2654\n",
      "Epoch 16/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1747 - acc: 0.2066 - val_loss: 0.0402 - val_acc: 0.2654\n",
      "Epoch 17/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1754 - acc: 0.2081 - val_loss: 0.0499 - val_acc: 0.2654\n",
      "Epoch 18/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1756 - acc: 0.2074 - val_loss: 0.0429 - val_acc: 0.2654\n",
      "Epoch 19/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1720 - acc: 0.2143 - val_loss: 0.0492 - val_acc: 0.2654\n",
      "Epoch 20/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1754 - acc: 0.2056 - val_loss: 0.0566 - val_acc: 0.2654\n",
      "Epoch 21/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1681 - acc: 0.2116 - val_loss: 0.0541 - val_acc: 0.2654\n",
      "Epoch 22/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1673 - acc: 0.2101 - val_loss: 0.0553 - val_acc: 0.2647\n",
      "Epoch 23/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1679 - acc: 0.2071 - val_loss: 0.0544 - val_acc: 0.2647\n",
      "Epoch 24/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1692 - acc: 0.2145 - val_loss: 0.0508 - val_acc: 0.2641\n",
      "Epoch 25/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1636 - acc: 0.2117 - val_loss: 0.0376 - val_acc: 0.2654\n",
      "Epoch 26/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1661 - acc: 0.2130 - val_loss: 0.0332 - val_acc: 0.2654\n",
      "Epoch 27/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1717 - acc: 0.2045 - val_loss: 0.0333 - val_acc: 0.2654\n",
      "Epoch 28/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1684 - acc: 0.2117 - val_loss: 0.0325 - val_acc: 0.2654\n",
      "Epoch 29/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1656 - acc: 0.2116 - val_loss: 0.0349 - val_acc: 0.2654\n",
      "Epoch 30/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1663 - acc: 0.2098 - val_loss: 0.0344 - val_acc: 0.2654\n",
      "Epoch 31/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1649 - acc: 0.2111 - val_loss: 0.0353 - val_acc: 0.2654\n",
      "Epoch 32/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1633 - acc: 0.2148 - val_loss: 0.0493 - val_acc: 0.2654\n",
      "Epoch 33/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1637 - acc: 0.2130 - val_loss: 0.0574 - val_acc: 0.2654\n",
      "Epoch 34/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1699 - acc: 0.2045 - val_loss: 0.0469 - val_acc: 0.2654\n",
      "Epoch 35/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1625 - acc: 0.2132 - val_loss: 0.0492 - val_acc: 0.2654\n",
      "Epoch 36/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1636 - acc: 0.2119 - val_loss: 0.0655 - val_acc: 0.2647\n",
      "Epoch 37/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1676 - acc: 0.2148 - val_loss: 0.0634 - val_acc: 0.2654\n",
      "Epoch 38/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1652 - acc: 0.2095 - val_loss: 0.0497 - val_acc: 0.2654\n",
      "Epoch 39/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1697 - acc: 0.2095 - val_loss: 0.0401 - val_acc: 0.2654\n",
      "Epoch 40/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1649 - acc: 0.2127 - val_loss: 0.0399 - val_acc: 0.2654\n",
      "Epoch 41/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1595 - acc: 0.2105 - val_loss: 0.0374 - val_acc: 0.2654\n",
      "Epoch 42/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1569 - acc: 0.2124 - val_loss: 0.0355 - val_acc: 0.2654\n",
      "Epoch 43/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1591 - acc: 0.2190 - val_loss: 0.0290 - val_acc: 0.2654\n",
      "Epoch 44/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1558 - acc: 0.2148 - val_loss: 0.0299 - val_acc: 0.2654\n",
      "Epoch 45/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1579 - acc: 0.2145 - val_loss: 0.0351 - val_acc: 0.2654\n",
      "Epoch 46/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1541 - acc: 0.2151 - val_loss: 0.0287 - val_acc: 0.2654\n",
      "Epoch 47/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1561 - acc: 0.2137 - val_loss: 0.0287 - val_acc: 0.2654\n",
      "Epoch 48/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1567 - acc: 0.2153 - val_loss: 0.0266 - val_acc: 0.2654\n",
      "Epoch 49/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1574 - acc: 0.2148 - val_loss: 0.0249 - val_acc: 0.2654\n",
      "Epoch 50/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1599 - acc: 0.2119 - val_loss: 0.0280 - val_acc: 0.2654\n",
      "Epoch 51/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1546 - acc: 0.2100 - val_loss: 0.0259 - val_acc: 0.2654\n",
      "Epoch 52/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1617 - acc: 0.2105 - val_loss: 0.0278 - val_acc: 0.2654\n",
      "Epoch 53/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1593 - acc: 0.2111 - val_loss: 0.0336 - val_acc: 0.2654\n",
      "Epoch 54/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1557 - acc: 0.2135 - val_loss: 0.0383 - val_acc: 0.2654\n",
      "Epoch 55/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1555 - acc: 0.2150 - val_loss: 0.0427 - val_acc: 0.2654\n",
      "Epoch 56/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1531 - acc: 0.2087 - val_loss: 0.0377 - val_acc: 0.2654\n",
      "Epoch 57/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1529 - acc: 0.2201 - val_loss: 0.0397 - val_acc: 0.2654\n",
      "Epoch 58/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1493 - acc: 0.2175 - val_loss: 0.0326 - val_acc: 0.2654\n",
      "Epoch 59/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1550 - acc: 0.2148 - val_loss: 0.0324 - val_acc: 0.2654\n",
      "Epoch 60/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1570 - acc: 0.2158 - val_loss: 0.0273 - val_acc: 0.2654\n",
      "Epoch 61/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1567 - acc: 0.2146 - val_loss: 0.0279 - val_acc: 0.2654\n",
      "Epoch 62/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1482 - acc: 0.2183 - val_loss: 0.0376 - val_acc: 0.2654\n",
      "Epoch 63/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1533 - acc: 0.2150 - val_loss: 0.0320 - val_acc: 0.2654\n",
      "Epoch 64/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1550 - acc: 0.2143 - val_loss: 0.0276 - val_acc: 0.2654\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6234/6234 [==============================] - 8s - loss: 0.1474 - acc: 0.2207 - val_loss: 0.0284 - val_acc: 0.2654\n",
      "Epoch 66/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1522 - acc: 0.2158 - val_loss: 0.0253 - val_acc: 0.2654\n",
      "Epoch 67/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1503 - acc: 0.2175 - val_loss: 0.0270 - val_acc: 0.2654\n",
      "Epoch 68/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1500 - acc: 0.2172 - val_loss: 0.0291 - val_acc: 0.2654\n",
      "Epoch 69/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1482 - acc: 0.2201 - val_loss: 0.0318 - val_acc: 0.2654\n",
      "Epoch 70/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1473 - acc: 0.2217 - val_loss: 0.0364 - val_acc: 0.2654\n",
      "Epoch 71/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1462 - acc: 0.2182 - val_loss: 0.0465 - val_acc: 0.2654\n",
      "Epoch 72/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1506 - acc: 0.2169 - val_loss: 0.0453 - val_acc: 0.2654\n",
      "Epoch 73/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1532 - acc: 0.2151 - val_loss: 0.0342 - val_acc: 0.2654\n",
      "Epoch 74/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1552 - acc: 0.2113 - val_loss: 0.0301 - val_acc: 0.2654\n",
      "Epoch 75/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1552 - acc: 0.2141 - val_loss: 0.0332 - val_acc: 0.2654\n",
      "Epoch 76/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1484 - acc: 0.2182 - val_loss: 0.0271 - val_acc: 0.2654\n",
      "Epoch 77/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1505 - acc: 0.2186 - val_loss: 0.0282 - val_acc: 0.2654\n",
      "Epoch 78/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1539 - acc: 0.2153 - val_loss: 0.0329 - val_acc: 0.2654\n",
      "Epoch 79/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1513 - acc: 0.2143 - val_loss: 0.0302 - val_acc: 0.2654\n",
      "Epoch 80/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1486 - acc: 0.2201 - val_loss: 0.0254 - val_acc: 0.2654\n",
      "Epoch 81/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1508 - acc: 0.2196 - val_loss: 0.0263 - val_acc: 0.2654\n",
      "Epoch 82/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1448 - acc: 0.2217 - val_loss: 0.0331 - val_acc: 0.2654\n",
      "Epoch 83/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1497 - acc: 0.2135 - val_loss: 0.0298 - val_acc: 0.2654\n",
      "Epoch 84/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1500 - acc: 0.2207 - val_loss: 0.0259 - val_acc: 0.2654\n",
      "Epoch 85/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1526 - acc: 0.2175 - val_loss: 0.0243 - val_acc: 0.2654\n",
      "Epoch 86/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1510 - acc: 0.2199 - val_loss: 0.0305 - val_acc: 0.2654\n",
      "Epoch 87/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1491 - acc: 0.2206 - val_loss: 0.0421 - val_acc: 0.2660\n",
      "Epoch 88/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1495 - acc: 0.2186 - val_loss: 0.0340 - val_acc: 0.2654\n",
      "Epoch 89/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1490 - acc: 0.2167 - val_loss: 0.0253 - val_acc: 0.2654\n",
      "Epoch 90/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1457 - acc: 0.2169 - val_loss: 0.0336 - val_acc: 0.2654\n",
      "Epoch 91/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1443 - acc: 0.2204 - val_loss: 0.0297 - val_acc: 0.2654\n",
      "Epoch 92/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1467 - acc: 0.2148 - val_loss: 0.0288 - val_acc: 0.2654\n",
      "Epoch 93/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1450 - acc: 0.2191 - val_loss: 0.0232 - val_acc: 0.2654\n",
      "Epoch 94/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1552 - acc: 0.2154 - val_loss: 0.0218 - val_acc: 0.2654\n",
      "Epoch 95/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1468 - acc: 0.2190 - val_loss: 0.0217 - val_acc: 0.2654\n",
      "Epoch 96/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1491 - acc: 0.2193 - val_loss: 0.0220 - val_acc: 0.2654\n",
      "Epoch 97/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1416 - acc: 0.2186 - val_loss: 0.0237 - val_acc: 0.2654\n",
      "Epoch 98/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1488 - acc: 0.2215 - val_loss: 0.0311 - val_acc: 0.2654\n",
      "Epoch 99/100\n",
      "6234/6234 [==============================] - 7s - loss: 0.1431 - acc: 0.2255 - val_loss: 0.0290 - val_acc: 0.2660\n",
      "Epoch 100/100\n",
      "6234/6234 [==============================] - 8s - loss: 0.1370 - acc: 0.2235 - val_loss: 0.0381 - val_acc: 0.2654\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=128, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=128, colour_space='RGB')\n",
    "\n",
    "t2 = Sequential()\n",
    "\n",
    "t2.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.5\n",
    "\n",
    "t2.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "t2.add(Dropout(keep_prob))\n",
    "\n",
    "t2.add(Flatten())\n",
    "t2.add(Dense(100))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Dense(50))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Dense(10))\n",
    "t2.add(Dropout(keep_prob))\n",
    "t2.add(Dense(1))\n",
    "\n",
    "t2.load_weights(\"./best_model.hdf5\")\n",
    "t2.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SaveEachEpoch(Callback):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.model.save('up-{}'.format(self.epoch))\n",
    "        self.epoch += 1\n",
    "\n",
    "callbacks_list = [SaveEachEpoch(t2)]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "t2.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=100,\n",
    "                     callbacks=callbacks_list)\n",
    "\n",
    "t2.save('./up_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bla bal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adamax, Adadelta, Adagrad, Adam, SGD, RMSprop\n",
    "\n",
    "samples = []\n",
    "with open('./data_4/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "with open('./data_4/driving_log_2.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "# log_csv\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "data_path = './data_4' # ./data\n",
    "ty = 40\n",
    "dy = -20\n",
    "\n",
    "def generator(samples, batch_size=32, colour_space='RGB'):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Get Images Path\n",
    "                center_name = data_path + '/IMG/' + batch_sample[0].split('/')[-1].strip()\n",
    "                left_name = data_path + '/IMG/' + batch_sample[1].split('/')[-1].strip()\n",
    "                right_name = data_path + '/IMG/'+ batch_sample[1].split('/')[-1].strip()\n",
    "                \n",
    "                # Read images from center, left and right cameras\n",
    "                center_image = cv2.imread(center_name)\n",
    "                left_image = cv2.imread(left_name)\n",
    "                right_image = cv2.imread(right_name)\n",
    "                \n",
    "                if colour_space == 'RGB':\n",
    "                    colour_conv = cv2.COLOR_BGR2RGB\n",
    "                else:\n",
    "                    colour_conv = cv2.COLOR_BGR2GRAY\n",
    "                    \n",
    "                # Convert Colour Space\n",
    "                center_image = cv2.cvtColor(center_image, colour_conv)\n",
    "                left_image = cv2.cvtColor(left_image, colour_conv)\n",
    "                right_image = cv2.cvtColor(right_image, colour_conv)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image = np.reshape(center_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    left_image = np.reshape(left_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "                    right_image = np.reshape(right_image, (center_image.shape[0], center_image.shape[1], 1))\n",
    "        \n",
    "                center_image = center_image[ty:dy,:, :]\n",
    "                left_image = left_image[ty:dy,:, :]\n",
    "                right_image = right_image[ty:dy,:, :]\n",
    "                \n",
    "                # Augment Images\n",
    "                center_image_flip = cv2.flip(center_image, 1)\n",
    "                left_image_flip = cv2.flip(left_image, 1)\n",
    "                right_image_flip = cv2.flip(right_image, 1)\n",
    "                \n",
    "                if colour_space != 'RGB':\n",
    "                    center_image_flip = np.reshape(center_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    left_image_flip = np.reshape(left_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                    right_image_flip = np.reshape(right_image_flip, (center_image_flip.shape[0], center_image_flip.shape[1], 1))\n",
    "                \n",
    "\n",
    "                # Get Measurements\n",
    "                center_angle = float(batch_sample[3])\n",
    "                # Create adjusted steering measurements for the side camera images\n",
    "                correction = 0.1 # this is a parameter to tune\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "                \n",
    "                # Augment Measurements\n",
    "                center_angle_flip = center_angle * -1.0\n",
    "                left_angle_flip = left_angle * -1.0\n",
    "                right_angle_flip = right_angle * -1.0\n",
    "                \n",
    "                \n",
    "                center_image = cv2.resize(center_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image = cv2.resize(left_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image = cv2.resize(right_image, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                center_image_flip = cv2.resize(center_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                left_image_flip = cv2.resize(left_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                right_image_flip = cv2.resize(right_image_flip, (200, 66), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)\n",
    "                \n",
    "                images.append(center_image_flip)\n",
    "                angles.append(center_angle_flip)\n",
    "                images.append(left_image_flip)\n",
    "                angles.append(left_angle_flip)\n",
    "                images.append(right_image_flip)\n",
    "                angles.append(right_angle_flip)\n",
    "                \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.0054Epoch 00000: val_loss improved from inf to 0.02369, saving model to lulu-00-0.02.hdf5\n",
      "84336/84336 [==============================] - 105s - loss: 0.0415 - acc: 0.0055 - val_loss: 0.0237 - val_acc: 0.0063\n",
      "Epoch 2/5\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.0055Epoch 00001: val_loss improved from 0.02369 to 0.01938, saving model to lulu-01-0.02.hdf5\n",
      "84336/84336 [==============================] - 97s - loss: 0.0275 - acc: 0.0055 - val_loss: 0.0194 - val_acc: 0.0063\n",
      "Epoch 3/5\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.0055Epoch 00002: val_loss improved from 0.01938 to 0.01724, saving model to lulu-02-0.02.hdf5\n",
      "84336/84336 [==============================] - 96s - loss: 0.0242 - acc: 0.0055 - val_loss: 0.0172 - val_acc: 0.0063\n",
      "Epoch 4/5\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.0056Epoch 00003: val_loss improved from 0.01724 to 0.01636, saving model to lulu-03-0.02.hdf5\n",
      "84336/84336 [==============================] - 97s - loss: 0.0230 - acc: 0.0056 - val_loss: 0.0164 - val_acc: 0.0063\n",
      "Epoch 5/5\n",
      "83712/84336 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.0056Epoch 00004: val_loss improved from 0.01636 to 0.01541, saving model to lulu-04-0.02.hdf5\n",
      "84336/84336 [==============================] - 98s - loss: 0.0214 - acc: 0.0056 - val_loss: 0.0154 - val_acc: 0.0063\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_samples, batch_size=128, colour_space='RGB')\n",
    "validation_generator = generator(validation_samples, batch_size=128, colour_space='RGB')\n",
    "\n",
    "bm = Sequential()\n",
    "\n",
    "bm.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66, 200, 3)))\n",
    "\n",
    "# k_init = TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "keep_prob = 0.2\n",
    "\n",
    "bm.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "bm.add(Dropout(keep_prob))\n",
    "\n",
    "bm.add(Flatten())\n",
    "bm.add(Dense(100))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Dense(50))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Dense(10))\n",
    "bm.add(Dropout(keep_prob))\n",
    "bm.add(Dense(1))\n",
    "\n",
    "bm.compile(loss='mse', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "filepath=\"lulu-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "n_cameras = 3\n",
    "n_augmentations = 2\n",
    "\n",
    "bm.fit_generator(train_generator, \n",
    "                     samples_per_epoch=len(train_samples) * n_cameras * n_augmentations, \n",
    "                     validation_data=validation_generator, \n",
    "                     nb_val_samples=len(validation_samples) * n_cameras * n_augmentations, \n",
    "                     nb_epoch=5,\n",
    "                     callbacks=callbacks_list)\n",
    "\n",
    "bm.save(\"./lulu.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
